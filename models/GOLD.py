from .base_model import BaseModel
import math
import os
from collections import OrderedDict

import torch
import torch.nn as nn
import torch.nn.functional as F

from .TripleEUNet import DualEUNet, TripleEUNet, LightweightTripleEUNet
from .base_model import BaseModel
from .loss import *
from .loss import HeterogeneousAttentionDistillationLoss, DifferenceAttentionLoss


class TripleHeteCD(BaseModel):
    def __init__(self, opt, is_train=True):
        """åˆå§‹åŒ–

        å‚æ•°:
            opt (Optionç±») -- å­˜å‚¨æ‰€æœ‰å®éªŒæ ‡å¿—çš„ç±»ï¼›éœ€è¦æ˜¯BaseOptionsçš„å­ç±»
        """
        BaseModel.__init__(self, opt, is_train=True)

        # æ˜¯å¦ä½¿ç”¨åŒç¼–ç å™¨ç½‘ç»œå’Œè’¸é¦å­¦ä¹ 
        self.use_distill = opt.use_distill
        # æ˜¯å¦ä½¿ç”¨è½»é‡åŒ–æ¨¡å‹
        self.use_lightweight = getattr(opt, 'use_lightweight', False)

        # æ·»åŠ åŠ¨æ€æƒé‡åˆ†é…ç›¸å…³å‚æ•°
        self.use_dynamic_weights = opt.use_dynamic_weights  # ç›´æ¥ä»optä¸­è·å–å‚æ•°å€¼
        self.weight_warmup_epochs = opt.weight_warmup_epochs  # æƒé‡çƒ­èº«é˜¶æ®µçš„è½®æ¬¡æ•°
        self.current_epoch = 0  # å½“å‰è®­ç»ƒè½®æ¬¡

        # åˆå§‹æƒé‡è®¾ç½®ï¼ˆä»»åŠ¡çº§ï¼‰
        self.init_cd_weight = opt.init_cd_weight
        self.init_distill_weight = opt.init_distill_weight
        self.init_diff_att_weight = opt.init_diff_att_weight

        # LCD å†…éƒ¨ï¼ˆå­¦ç”Ÿ/æ•™å¸ˆï¼‰åˆå§‹æƒé‡
        self.init_student_cd_weight = getattr(opt, 'init_student_cd_weight', 100.0)
        self.init_teacher_cd_weight = getattr(opt, 'init_teacher_cd_weight', 20.0)
        # LDISTILL å†…éƒ¨ï¼ˆç‰¹å¾/è¾“å‡ºï¼‰åˆå§‹æƒé‡
        self.init_feat_distill_weight = getattr(opt, 'init_feat_distill_weight', 0.7)
        self.init_out_distill_weight = getattr(opt, 'init_out_distill_weight', 0.3)
        # LA å†…éƒ¨ï¼ˆå·®å¼‚å›¾/é€šé“/ç©ºé—´ï¼‰åˆå§‹æƒé‡
        self.init_diff_map_weight = getattr(opt, 'init_diff_map_weight', 0.5)
        self.init_channel_att_weight = getattr(opt, 'init_channel_att_weight', 0.3)
        self.init_spatial_att_weight = getattr(opt, 'init_spatial_att_weight', 0.2)

        # CE ä¸ Dice åœ¨ LCD å†…éƒ¨çš„å›ºå®šæ¯”ä¾‹ï¼ˆé¿å…ä½¿ç”¨å˜åŒ–æ¯”ä¾‹åŠ¨æ€é¡¹ï¼‰
        self.ce_in_lcd_weight = getattr(opt, 'ce_in_lcd_weight', 100.0)
        self.dice_in_lcd_weight = getattr(opt, 'dice_in_lcd_weight', 150.0)

        # äº¤å‰ç†µç±»åˆ«æƒé‡ä¸è’¸é¦ç‰¹å¾æ©ç æƒé‡
        self.ce_weight_bg = getattr(opt, 'ce_weight_bg', 0.1)
        self.ce_weight_fg = getattr(opt, 'ce_weight_fg', 0.9)
        self.feature_mask_pos_weight = getattr(opt, 'feature_mask_pos_weight', 8.0)
        self.feature_mask_neg_weight = getattr(opt, 'feature_mask_neg_weight', 0.2)

        # æ•™å¸ˆç†µæ­£åˆ™ï¼ˆå¯é€‰ï¼Œé»˜è®¤å…³é—­ï¼‰
        self.teacher_entropy_weight = getattr(opt, 'teacher_entropy_weight', 0.0)

        # æŒ‡å®šè¦æ‰“å°çš„è®­ç»ƒæŸå¤±ã€‚è®­ç»ƒ/æµ‹è¯•è„šæœ¬å°†è°ƒç”¨<BaseModel.get_current_losses>
        self.loss_names = ['CD']
        if self.use_distill:
            self.loss_names.extend(['Distill', 'Diff_Att'])
        # åˆ†å±‚ä¸ç¡®å®šæ€§æƒé‡å‚æ•°
        if self.use_dynamic_weights:
            # ä»»åŠ¡çº§ï¼šLCD / LDISTILL / LA
            self.log_vars_task = nn.Parameter(torch.zeros(3))
            # LCD å†…éƒ¨ï¼šå­¦ç”ŸCD / æ•™å¸ˆCD
            self.log_vars_cd = nn.Parameter(torch.zeros(2))
            # LDISTILL å†…éƒ¨ï¼šç‰¹å¾è’¸é¦ / è¾“å‡ºè’¸é¦
            self.log_vars_distill = nn.Parameter(torch.zeros(2))
            # LA å†…éƒ¨ï¼šå·®å¼‚å›¾ / é€šé“æ³¨æ„åŠ› / ç©ºé—´æ³¨æ„åŠ›
            self.log_vars_att = nn.Parameter(torch.zeros(3))
        else:
            self.log_vars_task = None
            self.log_vars_cd = None
            self.log_vars_distill = None
            self.log_vars_att = None

        # æŒ‡å®šè¦ä¿å­˜/æ˜¾ç¤ºçš„å›¾åƒã€‚
        self.change_pred = None
        self.teacher_pred = None
        self.isTrain = is_train
        # æŒ‡å®šè¦ä¿å­˜åˆ°ç£ç›˜çš„æ¨¡å‹ã€‚
        self.model_names = ['CD']

        # å®šä¹‰ç½‘ç»œ
        if self.use_distill:
            if self.use_lightweight:
                # ä½¿ç”¨è½»é‡åŒ–åŒç¼–ç å™¨ç½‘ç»œ
                print("ä½¿ç”¨è½»é‡åŒ–æ¨¡å‹")
                self.netCD = LightweightTripleEUNet(
                    3, 2, 
                    channel_reduction=getattr(opt, 'channel_reduction', 0.5),
                    attention_reduction_ratio=getattr(opt, 'attention_reduction_ratio', 32)
                )
            else:
                # ä½¿ç”¨æ ‡å‡†åŒç¼–ç å™¨ç½‘ç»œ
                print("ä½¿ç”¨æ ‡å‡†æ¨¡å‹")
                self.netCD = TripleEUNet(3, 2)
                
            # ä½¿ç”¨è’¸é¦æŸå¤±ï¼ˆä»…è¿”å›ç‰¹å¾/è¾“å‡ºä¸¤éƒ¨åˆ†ï¼‰
            self.distill_loss = HeterogeneousAttentionDistillationLoss(
                temperature=getattr(opt, 'distill_temp', 2.0),
                reduction=getattr(opt, 'kl_div_reduction', 'batchmean')
            )
            # å·®å¼‚å›¾æ³¨æ„åŠ›è¿ç§»æŸå¤±ï¼ˆä»…ç”¨äºè®¡ç®—ä¸‰ä¸ªåŸå­é¡¹ï¼›æ€»å’Œåœ¨å¤–éƒ¨ç”¨ä¸ç¡®å®šæ€§æƒé‡èåˆï¼‰
            self.diff_att_loss = DifferenceAttentionLoss(
                reduction='mean',
                alpha=getattr(opt, 'diff_att_alpha', 0.5),
                beta=getattr(opt, 'diff_att_beta', 0.3),
                gamma=getattr(opt, 'diff_att_gamma', 0.2)
            )
        else:
            self.netCD = DualEUNet(3, 2)

        self.netCD.to(opt.gpu_ids[0])
        self.is_train = is_train

        if is_train:
            self.netCD = torch.nn.DataParallel(self.netCD, opt.gpu_ids)  # å¤šGPUæ”¯æŒ

        if self.isTrain:
            # å°†æ¨¡å‹ä¸log_varsæ·»åŠ åˆ°ä¼˜åŒ–å™¨ä¸­
            params = [
                {'params': filter(lambda p: p.requires_grad, self.netCD.parameters())},
            ]
            if self.use_dynamic_weights:
                params.append({'params': self.log_vars_task, 'lr': opt.lr * 0.1})
                params.append({'params': self.log_vars_cd, 'lr': opt.lr * 0.1})
                params.append({'params': self.log_vars_distill, 'lr': opt.lr * 0.1})
                params.append({'params': self.log_vars_att, 'lr': opt.lr * 0.1})
            self.optimizer_G = torch.optim.AdamW(params, lr=opt.lr,
                                                 betas=(0.9, 0.999), weight_decay=0.01)
            self.optimizers.append(self.optimizer_G)

    def set_epoch(self, epoch):
        """è®¾ç½®å½“å‰è®­ç»ƒè½®æ¬¡ï¼Œç”¨äºåŠ¨æ€æƒé‡è®¡ç®—

        å‚æ•°:
            epoch (int): å½“å‰è®­ç»ƒè½®æ¬¡
        """
        self.current_epoch = epoch

    def _compute_group_weights(self, log_vars, init_weights_tensor):
        """åŸºäºä¸ç¡®å®šæ€§çš„åˆ†ç»„æƒé‡è®¡ç®—ï¼Œæ”¯æŒwarmupä¸æŒ‰åˆå§‹é‡çº§ç¼©æ”¾"""
        if not self.use_dynamic_weights or log_vars is None:
            return init_weights_tensor
        
        # ç¡®ä¿log_varsä¸init_weights_tensoråœ¨åŒä¸€è®¾å¤‡ä¸Š
        log_vars = log_vars.to(init_weights_tensor.device)
        precision = torch.nn.functional.softplus(-log_vars) + 1e-8
        
        if self.current_epoch < self.weight_warmup_epochs:
            progress = self.current_epoch / max(1, self.weight_warmup_epochs)
            alpha = 0.5 * (1 - math.cos(progress * math.pi))
            fixed = init_weights_tensor / (init_weights_tensor.sum() + 1e-8)
            dynamic = precision / (precision.sum() + 1e-8)
            weights = (1 - alpha) * fixed + alpha * dynamic
        else:
            weights = precision / (precision.sum() + 1e-8)
        # å°†æƒé‡ç¼©æ”¾å›åˆå§‹é‡çº§ï¼ˆå»é™¤äºŒæ¬¡é€å…ƒç´ ç¼©æ”¾ï¼‰
        weights = weights * (init_weights_tensor.sum() + 1e-8)
        return weights

    def get_group_weights(self):
        """è¿”å›å››ç»„æƒé‡ï¼šä»»åŠ¡çº§ã€LCDå†…éƒ¨ã€LDISTILLå†…éƒ¨ã€LAå†…éƒ¨"""
        device = self.netCD.module.parameters().__next__().device if isinstance(self.netCD, torch.nn.DataParallel) else next(self.netCD.parameters()).device
        task_init = torch.tensor([self.init_cd_weight, self.init_distill_weight, self.init_diff_att_weight], device=device)
        cd_init = torch.tensor([self.init_student_cd_weight, self.init_teacher_cd_weight], device=device)
        distill_init = torch.tensor([self.init_feat_distill_weight, self.init_out_distill_weight], device=device)
        att_init = torch.tensor([self.init_diff_map_weight, self.init_channel_att_weight, self.init_spatial_att_weight], device=device)
        task_w = self._compute_group_weights(self.log_vars_task, task_init)
        cd_w = self._compute_group_weights(self.log_vars_cd, cd_init)
        distill_w = self._compute_group_weights(self.log_vars_distill, distill_init)
        att_w = self._compute_group_weights(self.log_vars_att, att_init)
        # ä»»åŠ¡çº§æƒé‡ clip ä¸ LCD ä¿åº•
        total = task_w.sum() + 1e-8
        min_lcd = 0.6
        max_distill = 0.3
        max_att = 0.2
        target = torch.tensor([min_lcd, max_distill, max_att], device=task_w.device) * total
        task_w = torch.stack([
            torch.max(task_w[0], target[0]),
            torch.min(task_w[1], target[1]),
            torch.min(task_w[2], target[2]),
        ])
        task_w = task_w / (task_w.sum() + 1e-8) * total
        return task_w, cd_w, distill_w, att_w

    def set_input(self, A, B, label, name, device, C=None):
        """ä»æ•°æ®åŠ è½½å™¨è§£åŒ…è¾“å…¥æ•°æ®å¹¶æ‰§è¡Œå¿…è¦çš„é¢„å¤„ç†æ­¥éª¤ã€‚

        å‚æ•°:
            A (tensor): æ—¶é—´ç‚¹1çš„å…‰å­¦å›¾åƒ
            B (tensor): æ—¶é—´ç‚¹2çš„SARå›¾åƒ
            label (tensor): å˜åŒ–æ£€æµ‹æ ‡ç­¾
            name (str): å›¾åƒåç§°
            device (torch.device): è®¾å¤‡
            C (tensor, optional): æ—¶é—´ç‚¹2çš„å…‰å­¦å›¾åƒï¼Œç”¨äºæ•™å¸ˆç½‘ç»œ
        """
        self.opt_img = A.to(device)
        self.sar_img = B.to(device)
        self.label = label.to(device)
        self.name = name

        # å¦‚æœæä¾›äº†æ—¶é—´ç‚¹2çš„å…‰å­¦å›¾åƒä¸”ä½¿ç”¨è’¸é¦å­¦ä¹ ï¼Œåˆ™å­˜å‚¨å®ƒ
        if C is not None and self.use_distill:
            self.opt_img2 = C.to(device)
        else:
            self.opt_img2 = None

    def load_weights(self, checkpoint_path):
        """åŠ è½½æ¨¡å‹æƒé‡

        å‚æ•°:
            checkpoint_path (str): æƒé‡æ–‡ä»¶çš„è·¯å¾„
        """
        checkpoint = torch.load(checkpoint_path)
        for key in list(checkpoint.keys()):
            if key.startswith('module.'):
                checkpoint[key[7:]] = checkpoint[key]
                del checkpoint[key]
        self.netCD.load_state_dict(checkpoint)
        if not self.isTrain:
            self.netCD.eval()
            print("å·²åŠ è½½æ¨¡å‹æƒé‡ï¼Œå¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼")

    def forward(self):
        """è¿è¡Œå‰å‘ä¼ æ’­ï¼›ç”±<optimize_parameters>å’Œ<test>å‡½æ•°è°ƒç”¨ã€‚"""
        [self.fake_B, self.fake_BB] = self.netCD(self.real_A, self.real_B)  # G(A)

    def get_val_pred(self):
        """è·å–éªŒè¯é›†çš„é¢„æµ‹ç»“æœ

        è¿”å›:
            tuple: åŒ…å«é¢„æµ‹ç»“æœå’Œç›¸åº”çš„æŸå¤±
        """
        self.netCD.eval()
        self.is_train = False
        with torch.no_grad():
            if self.use_distill and self.opt_img2 is not None:
                # å¯¹äºåŒç¼–ç å™¨ç½‘ç»œï¼Œåªè·å–å­¦ç”Ÿç½‘ç»œçš„è¾“å‡º
                self.change_pred = self.netCD(self.opt_img, self.sar_img, self.opt_img2, is_training=False)
            else:
                self.forward_CD()

            # ä½¿ç”¨ä¸è®­ç»ƒä¸€è‡´çš„ç±»æƒé‡
            cls_weights = torch.tensor([self.ce_weight_bg, self.ce_weight_fg]).cuda()
            loss_bn = CE_Loss(self.change_pred, self.label, cls_weights)

        self.is_train = True
        return self.change_pred, loss_bn

    def get_teacher_pred(self):
        """è·å–æ•™å¸ˆç½‘ç»œçš„é¢„æµ‹ç»“æœï¼Œä»…ç”¨äºéªŒè¯

        è¿”å›:
            tuple: åŒ…å«æ•™å¸ˆç½‘ç»œé¢„æµ‹ç»“æœå’Œç›¸åº”çš„æŸå¤±
        """
        if not self.use_distill or self.opt_img2 is None:
            return None, 0.0

        self.netCD.eval()
        with torch.no_grad():
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªå…‰å­¦å›¾åƒå’Œç¬¬äºŒä¸ªå…‰å­¦å›¾åƒè¿›è¡Œé¢„æµ‹
            student_out, teacher_out, _, _, _, _, _, _, _ = self.netCD(
                self.opt_img, self.sar_img, self.opt_img2, is_training=True
            )
            self.teacher_pred = teacher_out

            # ä½¿ç”¨ä¸è®­ç»ƒä¸€è‡´çš„ç±»æƒé‡
            cls_weights = torch.tensor([self.ce_weight_bg, self.ce_weight_fg]).cuda()
            loss_bn = CE_Loss(self.teacher_pred, self.label, cls_weights)

        return self.teacher_pred, loss_bn

    def forward_CD(self):
        """æ‰§è¡Œå˜åŒ–æ£€æµ‹çš„å‰å‘ä¼ æ’­"""
        if self.use_distill and self.opt_img2 is not None and self.is_train:
            # ä½¿ç”¨åŒç¼–ç å™¨ç½‘ç»œè¿›è¡Œè®­ç»ƒï¼Œè¿”å›å€¼åŒ…æ‹¬åŸå§‹ç‰¹å¾
            # (å­¦ç”Ÿè¾“å‡º, æ•™å¸ˆè¾“å‡º, å­¦ç”Ÿå¢å¼ºç‰¹å¾, æ•™å¸ˆå¢å¼ºç‰¹å¾, å­¦ç”Ÿä¸­é—´ç‰¹å¾, æ•™å¸ˆä¸­é—´ç‰¹å¾, å…‰å­¦t1ç‰¹å¾, å…‰å­¦t2ç‰¹å¾, SAR t2ç‰¹å¾)
            self.student_out, self.teacher_out, self.student_feat, self.teacher_feat, \
                self.student_mid_feat, self.teacher_mid_feat, self.opt_t1_feat, \
                self.opt_t2_feat, self.sar_t2_feat = self.netCD(
                self.opt_img, self.sar_img, self.opt_img2, is_training=True
            )
            self.change_pred = self.student_out
        else:
            # ä½¿ç”¨åŒåˆ†æ”¯ç½‘ç»œæˆ–è€…åŒç¼–ç å™¨ç½‘ç»œçš„æµ‹è¯•æ¨¡å¼
            self.change_pred = self.netCD(self.opt_img, self.sar_img)
        
        return self.change_pred

    def compute_losses(self):
        """è®¡ç®—æŸå¤±ä½†ä¸æ‰§è¡Œåå‘ä¼ æ’­ï¼Œç”¨äºä¸æ··åˆç²¾åº¦è®­ç»ƒé…åˆä½¿ç”¨"""
        self.change_pred = F.interpolate(self.change_pred, size=(self.opt_img.size(2), self.opt_img.size(3)),
                                         mode='bilinear', align_corners=True)
        # ç±»æƒé‡
        cls_weights = torch.tensor([self.ce_weight_bg, self.ce_weight_fg]).cuda()
        self.label = self.label.long()

        # ä¸»è¦å˜åŒ–æ£€æµ‹æŸå¤±ï¼ˆå­¦ç”Ÿï¼‰
        ce_loss = CE_Loss(self.change_pred, self.label, cls_weights=cls_weights)
        dice_loss = Dice_loss(self.change_pred, self.label)
        student_cd_loss = ce_loss * self.ce_in_lcd_weight + dice_loss * self.dice_in_lcd_weight

        # åˆå§‹åŒ–è’¸é¦ä¸æ³¨æ„åŠ›æŸå¤±
        self.loss_Distill = torch.tensor(0.0).cuda()
        self.loss_Diff_Att = torch.tensor(0.0).cuda()

        # æ•™å¸ˆç›‘ç£ï¼ˆåˆå¹¶å…¥ LCD å†…éƒ¨ï¼‰
        teacher_cd_loss = torch.tensor(0.0).cuda()
        if self.use_distill and hasattr(self, 'teacher_out') and self.teacher_out is not None:
            teacher_out_resized = F.interpolate(
                self.teacher_out,
                size=(self.change_pred.size(2), self.change_pred.size(3)),
                mode='bilinear',
                align_corners=True
            )
            teacher_ce_loss = CE_Loss(teacher_out_resized, self.label, cls_weights=cls_weights)
            teacher_dice_loss = Dice_loss(teacher_out_resized, self.label)
            teacher_cd_loss = teacher_ce_loss * self.ce_in_lcd_weight + teacher_dice_loss * self.dice_in_lcd_weight
            if self.teacher_entropy_weight > 0.0:
                teacher_probs = F.softmax(teacher_out_resized, dim=1)
                teacher_entropy = -torch.sum(teacher_probs * torch.log(teacher_probs + 1e-6), dim=1).mean()
                teacher_cd_loss = teacher_cd_loss + self.teacher_entropy_weight * teacher_entropy

            # æ„é€ è’¸é¦ç”¨çš„ç‰¹å¾æ©ç 
            if len(self.label.shape) == 3:
                label_mask = self.label.unsqueeze(1)
            else:
                label_mask = self.label
            feature_mask = torch.zeros_like(label_mask, dtype=torch.float)
            feature_mask[label_mask == 1] = self.feature_mask_pos_weight
            feature_mask[label_mask == 0] = self.feature_mask_neg_weight
            feature_mask = F.interpolate(
                feature_mask,
                size=self.student_feat.size()[2:],
                mode='nearest'
            )

            # å·®å¼‚å›¾æ³¨æ„åŠ›ä¸‰ä¸ªåŸå­é¡¹
            diff_att_total, diff_att_loss, channel_att_loss, spatial_att_loss = self.diff_att_loss(
                self.student_feat, self.teacher_feat,
                self.opt_t1_feat, self.opt_t2_feat, self.sar_t2_feat
            )

            # è’¸é¦ä¸¤ä¸ªå­é¡¹ï¼ˆç‰¹å¾/è¾“å‡ºï¼‰
            feat_loss, out_loss = self.distill_loss(
                self.student_feat,
                self.teacher_feat,
                self.change_pred,
                teacher_out_resized,
                self.opt_t1_feat,
                self.opt_t2_feat,
                self.sar_t2_feat,
                feature_mask
            )

            # è®¡ç®—åˆ†å±‚ä¸ç¡®å®šæ€§æƒé‡
            task_w, cd_w, distill_w, att_w = self.get_group_weights()

            # è’¸é¦ä¸æ•™å¸ˆç›‘ç£çƒ­èº«/æ¸å…¥
            if self.weight_warmup_epochs and self.weight_warmup_epochs > 0:
                progress = min(max(self.current_epoch / float(self.weight_warmup_epochs), 0.0), 1.0)
                distill_alpha = 0.5 * (1 - math.cos(progress * math.pi))
            else:
                distill_alpha = 1.0

            # ç»„åˆ LCDï¼ˆå­¦ç”Ÿ/æ•™å¸ˆï¼‰
            self.loss_CD = student_cd_loss * cd_w[0] + (teacher_cd_loss * distill_alpha) * cd_w[1]
            # ç»„åˆ LDISTILLï¼ˆç‰¹å¾/è¾“å‡ºï¼‰
            self.loss_Distill = distill_alpha * (feat_loss * distill_w[0] + out_loss * distill_w[1])
            # ç»„åˆ LAï¼ˆå·®å¼‚å›¾/é€šé“/ç©ºé—´ï¼‰
            self.loss_Diff_Att = (diff_att_loss * att_w[0] +
                                   channel_att_loss * att_w[1] +
                                   spatial_att_loss * att_w[2])

            # ä»»åŠ¡çº§èåˆ
            self.loss_G = (self.loss_CD * task_w[0] +
                           self.loss_Distill * task_w[1] +
                           self.loss_Diff_Att * task_w[2])
        else:
            # ä¸ä½¿ç”¨è’¸é¦æ—¶ï¼Œä»…æœ‰å­¦ç”Ÿ LCD
            task_w, cd_w, distill_w, att_w = self.get_group_weights()
            self.loss_CD = student_cd_loss * cd_w[0]  # ä»…å­¦ç”Ÿé¡¹
            self.loss_G = self.loss_CD * task_w[0]
            
        return self.loss_G

    def backward_G(self):
        """è®¡ç®—ç”Ÿæˆå™¨çš„æŸå¤±å¹¶è¿›è¡Œåå‘ä¼ æ’­"""
        self.compute_losses()  # è®¡ç®—æŸå¤±
        
        # æŸå¤±å¼‚å¸¸æ£€æµ‹
        if torch.isnan(self.loss_G) or torch.isinf(self.loss_G) or self.loss_G.item() > 1000:
            print(f"ğŸš¨ æ£€æµ‹åˆ°å¼‚å¸¸æŸå¤±å€¼: {self.loss_G.item()}")
            print(f"  - CDæŸå¤±: {self.loss_CD.item() if hasattr(self, 'loss_CD') else 'N/A'}")
            print(f"  - è’¸é¦æŸå¤±: {self.loss_Distill.item() if hasattr(self, 'loss_Distill') else 'N/A'}")
            print(f"  - æ³¨æ„åŠ›æŸå¤±: {self.loss_Diff_Att.item() if hasattr(self, 'loss_Diff_Att') else 'N/A'}")
            print(f"âš ï¸  ä¸ºé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œè·³è¿‡å½“å‰æ‰¹æ¬¡çš„åå‘ä¼ æ’­")
            return
        
        self.loss_G.backward()  # åå‘ä¼ æ’­

    def optimize_parameters(self, epoch):
        """ä¼˜åŒ–æ¨¡å‹å‚æ•°

        å‚æ•°:
            epoch (int): å½“å‰è®­ç»ƒè½®æ¬¡

        è¿”å›:
            tensor: å˜åŒ–æ£€æµ‹çš„é¢„æµ‹ç»“æœ
        """
        self.set_epoch(epoch)  # æ›´æ–°å½“å‰è®­ç»ƒè½®æ¬¡
        self.forward_CD()  # è®¡ç®—å‰å‘ä¼ æ’­
        self.optimizer_G.zero_grad()  # æ¸…ç©ºæ¢¯åº¦
        self.backward_G()  # è®¡ç®—æŸå¤±å¹¶åå‘ä¼ æ’­
        
        # æ·»åŠ æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        import torch.nn.utils
        gradient_clip_norm = getattr(self.opt, 'gradient_clip_norm', 0.5)
        torch.nn.utils.clip_grad_norm_(self.netCD.parameters(), max_norm=gradient_clip_norm)
        
        self.optimizer_G.step()  # æ›´æ–°å‚æ•°
        return self.change_pred  # è¿”å›å˜åŒ–æ£€æµ‹ç»“æœ

    def save_networks(self, epoch, save_best=False):
        """å°†æ‰€æœ‰ç½‘ç»œä¿å­˜åˆ°ç£ç›˜ï¼Œè¦†ç›–åŸºç±»æ–¹æ³•ä»¥æ”¯æŒä¿å­˜æœ€ä½³æ¨¡å‹

        å‚æ•°:
            epoch (int/str) -- å½“å‰epochæˆ–'best'ç­‰æ ‡è¯†
            save_best (bool) -- æ˜¯å¦å°†æ¨¡å‹ä¿å­˜ä¸ºæœ€ä½³æ¨¡å‹
        """
        for name in self.model_names:
            if isinstance(name, str):
                if save_best:
                    save_filename = 'best_net_%s.pth' % name
                else:
                    save_filename = '%s_net_%s.pth' % (epoch, name)
                save_path = os.path.join(self.save_dir, save_filename)
                net = getattr(self, 'net' + name)

                if len(self.gpu_ids) > 0 and torch.cuda.is_available():
                    # ç›´æ¥ä¿å­˜GPUä¸Šçš„æ¨¡å‹çŠ¶æ€å­—å…¸ï¼Œé¿å…ä¸å¿…è¦çš„GPU-CPUä¼ è¾“
                    torch.save(net.module.state_dict(), save_path)
                else:
                    torch.save(net.state_dict(), save_path)

                print(f'æ¨¡å‹å·²ä¿å­˜: {save_path}')

    def load_networks(self, epoch):
        """ä»ç£ç›˜åŠ è½½æ‰€æœ‰ç½‘ç»œï¼Œè¦†ç›–åŸºç±»æ–¹æ³•ä»¥æ”¯æŒåŠ è½½ç‰¹å®šepochæˆ–æœ€ä½³æ¨¡å‹

        å‚æ•°:
            epoch (int/str) -- å½“å‰epochæˆ–'best'/'latest'ç­‰æ ‡è¯†
        """
        for name in self.model_names:
            if isinstance(name, str):
                # å¦‚æœepochæ˜¯å®Œæ•´çš„æ–‡ä»¶åï¼ˆä¾‹å¦‚æ¥è‡ªtraining_info.txtï¼‰
                if isinstance(epoch, str) and epoch.endswith('_net_CD.pth'):
                    load_filename = epoch
                else:
                    load_filename = '%s_net_%s.pth' % (epoch, name)

                load_path = os.path.join(self.save_dir, load_filename)

                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if not os.path.exists(load_path):
                    print(f"è­¦å‘Š: æ¨¡å‹æ–‡ä»¶ {load_path} ä¸å­˜åœ¨ï¼")

                    # å°è¯•ä¸åŒçš„æƒ…å†µ
                    if epoch == 'latest' or (isinstance(epoch, str) and epoch.endswith('_net_CD.pth')):
                        print(f"å°è¯•æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶...")
                        # æ”¶é›†æ‰€æœ‰ç›¸å…³çš„æ¨¡å‹æ–‡ä»¶
                        model_files = [f for f in os.listdir(self.save_dir)
                                       if f.endswith(f'_net_{name}.pth') and not f.startswith('best')]

                        if model_files:
                            # æ ¹æ®æ–‡ä»¶åä¸­çš„æ•°å­—ï¼ˆé€šå¸¸æ˜¯epochï¼‰æ’åº
                            model_files.sort(key=lambda x: int(x.split('_')[0])
                            if x.split('_')[0].isdigit() else -1,
                                             reverse=True)

                            load_filename = model_files[0]
                            load_path = os.path.join(self.save_dir, load_filename)
                            print(f"å°†åŠ è½½æœ€æ–°æ¨¡å‹: {load_path}")
                        else:
                            print(f"æœªæ‰¾åˆ°ä»»ä½•æ™®é€šæ¨¡å‹æ–‡ä»¶ï¼Œå°è¯•æŸ¥æ‰¾æœ€ä½³æ¨¡å‹...")
                            # å¯»æ‰¾æœ€ä½³æ¨¡å‹
                            best_model = [f for f in os.listdir(self.save_dir)
                                          if f.startswith('best_net_') and f.endswith('.pth')]
                            if best_model:
                                load_filename = best_model[0]
                                load_path = os.path.join(self.save_dir, load_filename)
                                print(f"å°†åŠ è½½æœ€ä½³æ¨¡å‹: {load_path}")
                            else:
                                print(f"æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶ã€‚")

                # æœ€ç»ˆåŠ è½½æ¨¡å‹
                if os.path.exists(load_path):
                    net = getattr(self, 'net' + name)
                    state_dict = torch.load(load_path)
                    if isinstance(net, torch.nn.DataParallel):
                        net.module.load_state_dict(state_dict)
                    else:
                        net.load_state_dict(state_dict)
                    print(f"æ¨¡å‹å·²åŠ è½½: {load_path}")

    def get_current_losses(self):
        """è¿”å›å½“å‰æŸå¤±çš„æœ‰åºå­—å…¸"""
        losses = {}
        # åªè¿”å›loss_namesä¸­åˆ—å‡ºçš„æŸå¤±ï¼Œå¹¶ä¸”ç¡®ä¿è¯¥æŸå¤±ç¡®å®å­˜åœ¨
        for name in self.loss_names:
            loss_name = 'loss_' + name
            if hasattr(self, loss_name):
                loss_value = getattr(self, loss_name)
                # ç¡®ä¿æŸå¤±å€¼ä¸æ˜¯Noneä¸”ä¸æ˜¯0å¼ é‡
                if loss_value is not None and not (isinstance(loss_value, torch.Tensor) and loss_value.item() == 0):
                    losses[name] = loss_value.item()
        
        # æ·»åŠ æ•™å¸ˆç½‘ç»œæŸå¤±ä»¥ä¾¿ç›‘æ§
        if hasattr(self, 'teacher_loss'):
            losses['Teacher'] = self.teacher_loss.item()
            
        return losses
