import argparse
import glob
import math
import os
import random
import re
import shutil
from collections import defaultdict

import numpy as np
from PIL import Image, ImageOps
from tqdm import tqdm

import rasterio
from rasterio.transform import from_bounds

# æ–°å¢sklearnå¯¼å…¥ï¼Œç”¨äºåœ°ç†èšç±»
try:
    from sklearn.cluster import DBSCAN
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("è­¦å‘Š: sklearnæœªå®‰è£…ï¼Œåœ°ç†æ„ŸçŸ¥åŠŸèƒ½å°†è¢«ç¦ç”¨")

# é»˜è®¤å‚æ•°è®¾ç½®
DEFAULT_INPUT_DIR = r"C:\1DataSets\241120\Compare\Datas\Final"  # è¾“å…¥ç›®å½•
DEFAULT_OUTPUT_DIR = r"C:\1DataSets\241120\Compare\Datas\Split19"  # è¾“å‡ºç›®å½•
DEFAULT_TILE_SIZE = 512  # åˆ‡ç‰‡å¤§å°
DEFAULT_SIZE_TOLERANCE = 2  # å¤§å°å®¹å·®
DEFAULT_OVERLAP_RATIO = 0.5  # è£å‰ªé‡å æ¯”ä¾‹
DEFAULT_OVERLAP_THRESHOLD = 0.8  # é‡å åº¦é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼çš„å°å—å°†è¢«ä¸¢å¼ƒ
DEFAULT_VAL_RATIO = 0.2  # éªŒè¯é›†å æ€»æ•°æ®çš„æ¯”ä¾‹
DEFAULT_CREATE_TEST_FOLDER = True  # æ˜¯å¦åˆ›å»ºæµ‹è¯•é›†æ–‡ä»¶å¤¹
DEFAULT_FILTER_BLACK_TILES = True  # æ˜¯å¦è¿‡æ»¤çº¯é»‘è‰²å°å—
DEFAULT_BLACK_THRESHOLD = 0.95  # çº¯é»‘è‰²åˆ¤å®šé˜ˆå€¼ï¼Œè¶…è¿‡æ­¤æ¯”ä¾‹çš„é»‘è‰²åƒç´ å°†è¢«è§†ä¸ºçº¯é»‘è‰²å°å—

# åœ°ç†æ„ŸçŸ¥ç›¸å…³é»˜è®¤å‚æ•°
DEFAULT_GEO_AWARE = False  # æ˜¯å¦å¯ç”¨åœ°ç†æ„ŸçŸ¥åˆ’åˆ†
DEFAULT_GEO_EPS = 2000  # DBSCANèšç±»çš„é‚»åŸŸåŠå¾„ï¼ˆç±³ï¼‰
DEFAULT_GEO_MIN_SAMPLES = 1  # DBSCANèšç±»çš„æœ€å°æ ·æœ¬æ•°

# æ•°æ®å¢å¼ºæ–¹æ³•æ§åˆ¶
APPLY_H_FLIP = False    # æ˜¯å¦åº”ç”¨æ°´å¹³ç¿»è½¬
APPLY_V_FLIP = False    # æ˜¯å¦åº”ç”¨å‚ç›´ç¿»è½¬
APPLY_ROT90 = False     # æ˜¯å¦åº”ç”¨90Â°æ—‹è½¬
APPLY_ROT180 = False    # æ˜¯å¦åº”ç”¨180Â°æ—‹è½¬
APPLY_ROT270 = False    # æ˜¯å¦åº”ç”¨270Â°æ—‹è½¬

# ========================= æ–°å¢ï¼šå‰æ™¯ç»Ÿè®¡ä¸å‡è¡¡åˆ’åˆ†è¾…åŠ©å‡½æ•° ========================= #

def load_label_and_count_foreground(label_path: str):
    """
    è¯»å–æ•´å¹…æ ‡ç­¾å›¾å¹¶ç»Ÿè®¡å‰æ™¯åƒç´ æ•°é‡ä¸æ€»åƒç´ æ•°é‡ã€‚
    è§„åˆ™ï¼šåƒç´ å€¼ > 0 è§†ä¸ºå‰æ™¯ã€‚
    """
    try:
        img = Image.open(label_path).convert('L')
        arr = np.array(img)
        total = arr.shape[0] * arr.shape[1]
        fg = int((arr > 0).sum())
        return fg, total
    except Exception as e:
        print(f"è­¦å‘Š: æ— æ³•è¯»å–æ ‡ç­¾ {label_path} ç»Ÿè®¡å‰æ™¯: {e}")
        return 0, 0


def compute_fg_stats_for_basenames(input_dir: str, base_names: list):
    """
    ä¸ºæ¯ä¸ªåŸºç¡€åç§°ç»Ÿè®¡å‰æ™¯/æ€»åƒç´ ã€‚
    è¿”å›: dict[base_name] = { 'fg': int, 'total': int }
    """
    stats = {}
    for base_name in base_names:
        label_path = os.path.join(input_dir, f"{base_name}_E.png")
        fg, total = load_label_and_count_foreground(label_path)
        stats[base_name] = {'fg': fg, 'total': total}
    return stats


def aggregate_cluster_stats(base_names: list, cluster_labels: np.ndarray, fg_stats: dict):
    """
    èšåˆæ¯ä¸ªèšç±»çš„å›¾åƒæ•°é‡ä¸å‰æ™¯/æ€»åƒç´ ç»Ÿè®¡ã€‚
    è¿”å›: dict[label] = { 'indices': np.ndarray, 'names': list, 'count': int, 'fg': int, 'total': int }
    """
    cluster_info = {}
    unique_labels = np.unique(cluster_labels)
    for label in unique_labels:
        indices = np.where(cluster_labels == label)[0]
        names = [base_names[i] for i in indices]
        fg_sum = 0
        total_sum = 0
        for name in names:
            s = fg_stats.get(name, {'fg': 0, 'total': 0})
            fg_sum += s['fg']
            total_sum += s['total']
        cluster_info[label] = {
            'indices': indices,
            'names': names,
            'count': len(names),
            'fg': fg_sum,
            'total': total_sum,
        }
    return cluster_info


def print_split_fg_summary(split_name: str, names: list, fg_stats: dict):
    """
    æ‰“å°æŸä¸ªåˆ’åˆ†çš„å‰æ™¯æ¯”ä¾‹ç»Ÿè®¡ã€‚
    """
    fg_total = 0
    pix_total = 0
    for n in names:
        s = fg_stats.get(n, {'fg': 0, 'total': 0})
        fg_total += s['fg']
        pix_total += s['total']
    ratio = (fg_total / pix_total) if pix_total > 0 else 0.0
    print(f"{split_name} å‰æ™¯åƒç´ : {fg_total} / {pix_total} (æ¯”ä¾‹: {ratio*100:.2f}%)")


# ========================= ç°æœ‰å‡½æ•° ========================= #

def get_geo_transform(tif_path):
    """
    è·å–TIFæ–‡ä»¶çš„åœ°ç†åæ ‡å˜æ¢ä¿¡æ¯
    
    å‚æ•°:
        tif_path: TIFæ–‡ä»¶è·¯å¾„
    
    è¿”å›:
        åœ°ç†åæ ‡å˜æ¢çŸ©é˜µ
    """
    try:
        with rasterio.open(tif_path) as src:
            return src.transform
    except Exception as e:
        raise RuntimeError(f"æ— æ³•è¯»å– {tif_path} çš„åœ°ç†åæ ‡ä¿¡æ¯: {e}")


def get_image_center_coordinates(tif_path):
    """
    è·å–TIFæ–‡ä»¶çš„ä¸­å¿ƒåœ°ç†åæ ‡
    
    å‚æ•°:
        tif_path: TIFæ–‡ä»¶è·¯å¾„
    
    è¿”å›:
        (center_x, center_y) ä¸­å¿ƒåœ°ç†åæ ‡
    """
    try:
        with rasterio.open(tif_path) as src:
            bounds = src.bounds
            center_x = (bounds.left + bounds.right) / 2
            center_y = (bounds.bottom + bounds.top) / 2
            return center_x, center_y
    except Exception as e:
        print(f"è­¦å‘Š: æ— æ³•è¯»å– {tif_path} çš„åœ°ç†åæ ‡: {e}")
        return None, None


def analyze_geographic_distribution(input_dir, base_names):
    """
    åˆ†ææ‰€æœ‰åŸå§‹å¤§å›¾çš„åœ°ç†åæ ‡åˆ†å¸ƒ
    
    å‚æ•°:
        input_dir: è¾“å…¥ç›®å½•
        base_names: åŸºç¡€åç§°åˆ—è¡¨
    
    è¿”å›:
        coords: numpyæ•°ç»„ï¼ŒåŒ…å«æ‰€æœ‰å›¾åƒçš„åœ°ç†åæ ‡
        valid_names: æœ‰æ•ˆçš„åŸºç¡€åç§°åˆ—è¡¨ï¼ˆèƒ½è¯»å–åœ°ç†åæ ‡çš„ï¼‰
    """
    coords = []
    valid_names = []
    
    print("åˆ†æåœ°ç†åæ ‡åˆ†å¸ƒ...")
    for base_name in tqdm(base_names, desc="è¯»å–åœ°ç†åæ ‡"):
        tif_path = os.path.join(input_dir, f"{base_name}_A.tif")
        center_x, center_y = get_image_center_coordinates(tif_path)
        
        if center_x is not None and center_y is not None:
            coords.append([center_x, center_y])
            valid_names.append(base_name)
        else:
            print(f"è·³è¿‡æ— æ³•è¯»å–åœ°ç†åæ ‡çš„å›¾åƒ: {base_name}")
    
    coords = np.array(coords)
    
    if len(coords) > 0:
        print(f"æˆåŠŸè·å– {len(coords)} ä¸ªå›¾åƒçš„åœ°ç†åæ ‡")
        print(f"åæ ‡èŒƒå›´: X({coords[:, 0].min():.2f} ~ {coords[:, 0].max():.2f}), Y({coords[:, 1].min():.2f} ~ {coords[:, 1].max():.2f})")
        
        # è®¡ç®—æœ€è¿‘é‚»è·ç¦»ç»Ÿè®¡
        if len(coords) > 1:
            from scipy.spatial.distance import pdist
            distances = pdist(coords)
            print(f"å›¾åƒé—´è·ç¦»ç»Ÿè®¡: æœ€å°{distances.min():.0f}m, ä¸­ä½æ•°{np.median(distances):.0f}m, æœ€å¤§{distances.max():.0f}m")
    
    return coords, valid_names


def perform_geographic_clustering(coords, eps=2000, min_samples=1):
    """
    å¯¹åœ°ç†åæ ‡è¿›è¡ŒDBSCANèšç±»
    
    å‚æ•°:
        coords: åœ°ç†åæ ‡æ•°ç»„
        eps: èšç±»é‚»åŸŸåŠå¾„ï¼ˆç±³ï¼‰
        min_samples: æœ€å°æ ·æœ¬æ•°
    
    è¿”å›:
        cluster_labels: èšç±»æ ‡ç­¾æ•°ç»„
    """
    if not SKLEARN_AVAILABLE:
        print("è­¦å‘Š: sklearnä¸å¯ç”¨ï¼Œè¿”å›éšæœºèšç±»æ ‡ç­¾")
        return np.random.randint(0, 2, len(coords))
    
    print(f"æ‰§è¡Œåœ°ç†èšç±» (é‚»åŸŸåŠå¾„: {eps}m, æœ€å°æ ·æœ¬æ•°: {min_samples})...")
    
    # ä½¿ç”¨DBSCANè¿›è¡Œèšç±»
    clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(coords)
    cluster_labels = clustering.labels_
    
    # ç»Ÿè®¡èšç±»ç»“æœ
    unique_labels = np.unique(cluster_labels)
    n_clusters = len(unique_labels) - (1 if -1 in cluster_labels else 0)
    n_noise = list(cluster_labels).count(-1)
    
    print(f"èšç±»ç»“æœ: {n_clusters} ä¸ªèšç±», {n_noise} ä¸ªå™ªå£°ç‚¹")
    
    for label in unique_labels:
        if label == -1:
            continue  # è·³è¿‡å™ªå£°ç‚¹
        cluster_size = list(cluster_labels).count(label)
        print(f"  èšç±» {label}: {cluster_size} ä¸ªå›¾åƒ")
    
    return cluster_labels


def geo_aware_train_val_split(base_names, coords, cluster_labels, val_ratio=0.2):
    """
    åŸºäºåœ°ç†èšç±»è¿›è¡Œè®­ç»ƒ/éªŒè¯é›†åˆ’åˆ†ï¼Œç¡®ä¿åœ°ç†å®Œå…¨åˆ†ç¦»
    
    å‚æ•°:
        base_names: åŸºç¡€åç§°åˆ—è¡¨
        coords: åœ°ç†åæ ‡æ•°ç»„
        cluster_labels: èšç±»æ ‡ç­¾
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
    
    è¿”å›:
        train_names: è®­ç»ƒé›†åŸºç¡€åç§°åˆ—è¡¨
        val_names: éªŒè¯é›†åŸºç¡€åç§°åˆ—è¡¨
    """
    print("æ‰§è¡Œåœ°ç†æ„ŸçŸ¥çš„è®­ç»ƒ/éªŒè¯é›†åˆ’åˆ†...")
    
    # ç»Ÿè®¡èšç±»ç»“æœï¼ˆè¡¥å……å‰æ™¯ç»Ÿè®¡ï¼‰
    unique_labels = np.unique(cluster_labels)

    # å…ˆè®¡ç®—æ¯å¼ å›¾çš„å‰æ™¯ç»Ÿè®¡
    input_dir_placeholder = None  # ä»…ä¸ºç­¾åå…¼å®¹å ä½ï¼Œæ­¤å‡½æ•°å†…ä¸ç›´æ¥è®¿é—®ç£ç›˜
    # æ³¨æ„ï¼šçœŸæ­£çš„ç»Ÿè®¡åœ¨å¤–éƒ¨å·²å®Œæˆï¼Œå¹¶åœ¨è°ƒç”¨æ—¶æ³¨å…¥ã€‚ä¸ºæœ€å°ä¾µå…¥æ”¹åŠ¨ï¼Œæˆ‘ä»¬åœ¨æœ¬å‡½æ•°å†…é‡å»ºä¸€æ¬¡ç»Ÿè®¡ã€‚
    # ç”±äºåŸå‡½æ•°ç­¾åä¸å«è¾“å…¥è·¯å¾„ï¼Œè¿™é‡Œé‡‡ç”¨å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨å…¨å±€å˜é‡ç¼“å­˜ã€‚
    # ä¸ºé¿å…å¼•å…¥å…¨å±€çŠ¶æ€ï¼Œåç»­åœ¨è°ƒç”¨å¤„ç›´æ¥æ›¿æ¢ä¸ºæ–°ç‰ˆå‡½æ•° geo_aware_train_val_split_balancedã€‚

    # ä¸ºä¿æŒå‘åå…¼å®¹ï¼Œæ­¤å¤„ä¿ç•™åŸå§‹ç®€å•ç­–ç•¥ï¼ˆä»¥é˜²å¤–éƒ¨æœªæ›¿æ¢è°ƒç”¨ï¼‰ã€‚
    cluster_info = {}
    for label in unique_labels:
        indices = np.where(cluster_labels == label)[0]
        cluster_info[label] = {
            'indices': indices,
            'count': list(cluster_labels).count(label),
            'names': [base_names[i] for i in indices]
        }

    # åŸå§‹ç®€å•è´ªå¿ƒï¼šæ•´ç°‡åˆ’åˆ†ï¼Œå°½é‡æ¥è¿‘ç›®æ ‡æ•°é‡
    total_images = len(base_names)
    target_val_size = int(total_images * val_ratio)
    print(f"ç›®æ ‡éªŒè¯é›†å¤§å°: {target_val_size}/{total_images} ({val_ratio*100:.1f}%)")

    sorted_clusters = sorted(cluster_info.items(), key=lambda x: x[1]['count'], reverse=True)
    train_names = []
    val_names = []
    current_val_size = 0

    for cluster_label, info in sorted_clusters:
        if cluster_label == -1:
            # å™ªå£°ç‚¹å‡åŒ€åˆ†é…ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
            noise_names = info['names']
            random.shuffle(noise_names)
            remaining_val_need = target_val_size - current_val_size
            noise_for_val = min(remaining_val_need, len(noise_names) // 2)
            val_names.extend(noise_names[:noise_for_val])
            train_names.extend(noise_names[noise_for_val:])
            current_val_size += noise_for_val
            print(f"  å™ªå£°ç‚¹: {len(noise_names)} ä¸ªå›¾åƒ -> è®­ç»ƒé›†{len(noise_names)-noise_for_val}, éªŒè¯é›†{noise_for_val}")
        else:
            if current_val_size < target_val_size and current_val_size + info['count'] <= target_val_size * 1.2:
                val_names.extend(info['names'])
                current_val_size += info['count']
                print(f"  èšç±» {cluster_label}: {info['count']} ä¸ªå›¾åƒ -> éªŒè¯é›†")
            else:
                train_names.extend(info['names'])
                print(f"  èšç±» {cluster_label}: {info['count']} ä¸ªå›¾åƒ -> è®­ç»ƒé›†")

    print(f"\nåœ°ç†æ„ŸçŸ¥åˆ’åˆ†ç»“æœ:")
    print(f"  è®­ç»ƒé›†: {len(train_names)} ä¸ªå›¾åƒ")
    print(f"  éªŒè¯é›†: {len(val_names)} ä¸ªå›¾åƒ")

    return train_names, val_names


# ========================= æ–°å¢ï¼šå‡è¡¡å‰æ™¯æ¯”ä¾‹çš„åœ°ç†åˆ’åˆ† ========================= #

def geo_aware_train_val_split_balanced(base_names, cluster_labels, val_ratio, fg_stats):
    """
    åŸºäºåœ°ç†èšç±»è¿›è¡Œè®­ç»ƒ/éªŒè¯é›†åˆ’åˆ†ï¼Œå¹¶å°½é‡åŒ¹é…æ•´ä½“å‰æ™¯æ¯”ä¾‹ã€‚

    å‚æ•°:
        base_names: åŸºç¡€åç§°åˆ—è¡¨
        cluster_labels: èšç±»æ ‡ç­¾
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        fg_stats: dict[base_name] -> {'fg': int, 'total': int}

    è¿”å›:
        train_names, val_names
    """
    print("æ‰§è¡Œåœ°ç†æ„ŸçŸ¥çš„è®­ç»ƒ/éªŒè¯é›†åˆ’åˆ†ï¼ˆå‡è¡¡å‰æ™¯æ¯”ä¾‹ï¼‰...")

    # è®¡ç®—èšç±»çº§æ±‡æ€»
    clusters = aggregate_cluster_stats(base_names, cluster_labels, fg_stats)

    total_images = len(base_names)
    target_val_size = int(total_images * val_ratio)

    # å…¨å±€å‰æ™¯æ¯”ä¾‹ï¼ˆä»¥åƒç´ è®¡ï¼‰
    global_fg = sum(fg_stats[n]['fg'] for n in base_names if n in fg_stats)
    global_total = sum(fg_stats[n]['total'] for n in base_names if n in fg_stats)
    global_ratio = (global_fg / global_total) if global_total > 0 else 0.0
    print(f"å…¨å±€å‰æ™¯æ¯”ä¾‹(åƒç´ ): {global_fg}/{global_total} = {global_ratio*100:.2f}%")

    # è´ªå¿ƒï¼šæŒ‰ç°‡å¤§å°é™åºéå†ï¼Œå°†èƒ½è®©éªŒè¯é›†å‰æ™¯æ¯”ä¾‹æ›´æ¥è¿‘å…¨å±€æ¯”ä¾‹çš„ç°‡ä¼˜å…ˆåŠ å…¥éªŒè¯é›†
    sorted_clusters = sorted(clusters.items(), key=lambda kv: kv[1]['count'], reverse=True)

    train_names, val_names = [], []
    val_fg, val_total, val_count = 0, 0, 0

    for label, info in sorted_clusters:
        names = info['names']
        c_count = info['count']
        c_fg = info['fg']
        c_total = info['total']

        # è‹¥éªŒè¯é›†å°šæœªè¾¾åˆ°ç›®æ ‡æ•°é‡ï¼Œåˆ™è€ƒè™‘åŠ å…¥éªŒè¯é›†
        if val_count < target_val_size:
            new_val_count = val_count + c_count
            new_val_fg = val_fg + c_fg
            new_val_total = val_total + c_total
            new_ratio = (new_val_fg / new_val_total) if new_val_total > 0 else 0.0
            curr_ratio = (val_fg / val_total) if val_total > 0 else 0.0

            improve = abs(new_ratio - global_ratio) < abs(curr_ratio - global_ratio)
            within_limit = (new_val_count <= int(target_val_size * 1.2))

            if within_limit and (improve or (target_val_size - val_count) >= c_count):
                val_names.extend(names)
                val_fg, val_total, val_count = new_val_fg, new_val_total, new_val_count
                print(f"  èšç±» {label}: {c_count} -> éªŒè¯é›† (val_ratio: {new_ratio*100:.2f}%)")
            else:
                train_names.extend(names)
                print(f"  èšç±» {label}: {c_count} -> è®­ç»ƒé›†")
        else:
            train_names.extend(names)
            print(f"  èšç±» {label}: {c_count} -> è®­ç»ƒé›†")

    # å¦‚éªŒè¯é›†ä¸è¶³ï¼Œå›å¡«æœ€å°ç°‡
    if val_count < target_val_size:
        remaining = target_val_size - val_count
        leftovers = [ (label, info) for label, info in sorted_clusters if info['names'][0] in train_names or True ]
        # ç®€åŒ–å¤„ç†ï¼šæŒ‰ç°‡å¤§å°å‡åºå›å¡«
        leftovers_sorted = sorted(clusters.items(), key=lambda kv: kv[1]['count'])
        for label, info in leftovers_sorted:
            names = [n for n in info['names'] if n in train_names]
            if not names:
                continue
            if val_count + len(names) <= target_val_size * 1.2:
                for n in names:
                    train_names.remove(n)
                val_names.extend(names)
                val_count += len(names)
                print(f"  å›å¡«èšç±» {label}: {len(names)} å¼  -> éªŒè¯é›†")
            if val_count >= target_val_size:
                break

    print(f"\nåœ°ç†æ„ŸçŸ¥(å‡è¡¡)åˆ’åˆ†ç»“æœ:")
    print(f"  è®­ç»ƒé›†: {len(train_names)} ä¸ªå›¾åƒ")
    print(f"  éªŒè¯é›†: {len(val_names)} ä¸ªå›¾åƒ")

    return train_names, val_names


def verify_geographic_separation(train_names, val_names, input_dir, min_distance=1000):
    """
    éªŒè¯è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¹‹é—´çš„åœ°ç†åˆ†ç¦»ç¨‹åº¦
    
    å‚æ•°:
        train_names: è®­ç»ƒé›†åŸºç¡€åç§°åˆ—è¡¨
        val_names: éªŒè¯é›†åŸºç¡€åç§°åˆ—è¡¨
        input_dir: è¾“å…¥ç›®å½•
        min_distance: æœ€å°è·ç¦»é˜ˆå€¼ï¼ˆç±³ï¼‰
    
    è¿”å›:
        separation_ok: æ˜¯å¦æ»¡è¶³åœ°ç†åˆ†ç¦»è¦æ±‚
        min_distance_found: å®é™…æ‰¾åˆ°çš„æœ€å°è·ç¦»
    """
    print("éªŒè¯åœ°ç†åˆ†ç¦»ç¨‹åº¦...")
    
    # è·å–è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„åæ ‡
    train_coords = []
    val_coords = []
    
    for name in train_names:
        tif_path = os.path.join(input_dir, f"{name}_A.tif")
        center_x, center_y = get_image_center_coordinates(tif_path)
        if center_x is not None:
            train_coords.append([center_x, center_y])
    
    for name in val_names:
        tif_path = os.path.join(input_dir, f"{name}_A.tif")
        center_x, center_y = get_image_center_coordinates(tif_path)
        if center_x is not None:
            val_coords.append([center_x, center_y])
    
    if not train_coords or not val_coords:
        print("è­¦å‘Š: æ— æ³•è·å–åæ ‡ä¿¡æ¯ï¼Œè·³è¿‡åˆ†ç¦»éªŒè¯")
        return True, float('inf')
    
    train_coords = np.array(train_coords)
    val_coords = np.array(val_coords)
    
    # è®¡ç®—è®­ç»ƒé›†å’ŒéªŒè¯é›†é—´çš„æœ€å°è·ç¦»
    from scipy.spatial.distance import cdist
    distances = cdist(train_coords, val_coords)
    min_distance_found = distances.min()
    
    separation_ok = min_distance_found >= min_distance
    
    print(f"  è®­ç»ƒé›†ä¸éªŒè¯é›†æœ€å°è·ç¦»: {min_distance_found:.0f}m")
    print(f"  åœ°ç†åˆ†ç¦»çŠ¶æ€: {'âœ“ æ»¡è¶³è¦æ±‚' if separation_ok else 'âœ— è·ç¦»è¿‡è¿‘'} (é˜ˆå€¼: {min_distance}m)")
    
    return separation_ok, min_distance_found


def pixel_to_geo_coords(pixel_x, pixel_y, transform):
    """
    å°†åƒç´ åæ ‡è½¬æ¢ä¸ºåœ°ç†åæ ‡
    
    å‚æ•°:
        pixel_x, pixel_y: åƒç´ åæ ‡
        transform: rasterioå˜æ¢çŸ©é˜µ
    
    è¿”å›:
        (geo_x, geo_y) åœ°ç†åæ ‡
    """
    geo_x = transform[2] + pixel_x * transform[0] + pixel_y * transform[1]
    geo_y = transform[5] + pixel_x * transform[3] + pixel_y * transform[4]
    return geo_x, geo_y


def pixel_box_to_geo_box(pixel_box, transform):
    """
    å°†åƒç´ åæ ‡è¾¹ç•Œæ¡†è½¬æ¢ä¸ºåœ°ç†åæ ‡è¾¹ç•Œæ¡†
    
    å‚æ•°:
        pixel_box: (x1, y1, x2, y2) åƒç´ åæ ‡è¾¹ç•Œæ¡†
        transform: rasterioå˜æ¢çŸ©é˜µ
    
    è¿”å›:
        (geo_x1, geo_y1, geo_x2, geo_y2) åœ°ç†åæ ‡è¾¹ç•Œæ¡†
    """
    x1, y1, x2, y2 = pixel_box
    
    # è½¬æ¢å››ä¸ªè§’ç‚¹
    geo_x1, geo_y1 = pixel_to_geo_coords(x1, y1, transform)
    geo_x2, geo_y2 = pixel_to_geo_coords(x2, y2, transform)
    
    # ç¡®ä¿åæ ‡é¡ºåºæ­£ç¡®ï¼ˆå·¦ä¸Šè§’åˆ°å³ä¸‹è§’ï¼‰
    min_x, max_x = min(geo_x1, geo_x2), max(geo_x1, geo_x2)
    min_y, max_y = min(geo_y1, geo_y2), max(geo_y1, geo_y2)
    
    return (min_x, min_y, max_x, max_y)


def is_black_tile(tile, threshold=0.95):
    """
    æ£€æµ‹å›¾åƒå°å—æ˜¯å¦ä¸ºçº¯é»‘è‰²æˆ–æ¥è¿‘çº¯é»‘è‰²
    
    å‚æ•°:
        tile: PILå›¾åƒå¯¹è±¡
        threshold: é»‘è‰²åƒç´ æ¯”ä¾‹é˜ˆå€¼ï¼ˆ0-1ä¹‹é—´ï¼‰
    
    è¿”å›:
        Trueå¦‚æœæ˜¯çº¯é»‘è‰²å°å—ï¼ŒFalseå¦åˆ™
    """
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    img_array = np.array(tile)
    
    # è®¡ç®—æ€»åƒç´ æ•°
    if len(img_array.shape) == 2:  # ç°åº¦å›¾åƒ
        total_pixels = img_array.size
        # è®¡ç®—é»‘è‰²åƒç´ æ•°é‡ï¼ˆå€¼ä¸º0æˆ–æ¥è¿‘0çš„åƒç´ ï¼‰
        black_pixels = np.sum(img_array <= 5)  # å…è®¸ä¸€äº›å™ªå£°ï¼Œå€¼<=5è®¤ä¸ºæ˜¯é»‘è‰²
    else:  # å½©è‰²å›¾åƒ
        total_pixels = img_array.shape[0] * img_array.shape[1]
        # è®¡ç®—é»‘è‰²åƒç´ æ•°é‡ï¼ˆRGBæ‰€æœ‰é€šé“éƒ½æ¥è¿‘0çš„åƒç´ ï¼‰
        black_mask = np.all(img_array <= 5, axis=2)  # RGBæ‰€æœ‰é€šé“éƒ½<=5
        black_pixels = np.sum(black_mask)
    
    # è®¡ç®—é»‘è‰²åƒç´ æ¯”ä¾‹
    black_ratio = black_pixels / total_pixels
    
    return black_ratio >= threshold


def calculate_overlap_ratio(box1, box2):
    """
    è®¡ç®—ä¸¤ä¸ªçŸ©å½¢æ¡†çš„æ¯”ä¾‹
    
    å‚æ•°:
        box1: ç¬¬ä¸€ä¸ªçŸ©å½¢æ¡† (x1, y1, x2, y2)
        box2: ç¬¬äºŒä¸ªçŸ©å½¢æ¡† (x1, y1, x2, y2)
    
    è¿”å›:
        é‡å æ¯”ä¾‹ (0-1ä¹‹é—´)
    """
    x1_1, y1_1, x2_1, y2_1 = box1
    x1_2, y1_2, x2_2, y2_2 = box2
    
    # è®¡ç®—äº¤é›†
    x1_inter = max(x1_1, x1_2)
    y1_inter = max(y1_1, y1_2)
    x2_inter = min(x2_1, x2_2)
    y2_inter = min(y2_1, y2_2)
    
    if x2_inter <= x1_inter or y2_inter <= y1_inter:
        return 0.0
    
    inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)
    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
    
    # ä½¿ç”¨è¾ƒå°åŒºåŸŸä½œä¸ºåˆ†æ¯è®¡ç®—é‡å æ¯”ä¾‹
    smaller_area = min(area1, area2)
    overlap_ratio = inter_area / smaller_area if smaller_area > 0 else 0.0
    
    return overlap_ratio


def check_overlap_with_existing(new_box, existing_boxes, threshold=0.8):
    """
    æ£€æŸ¥æ–°çš„çŸ©å½¢æ¡†æ˜¯å¦ä¸å·²å­˜åœ¨çš„çŸ©å½¢æ¡†é‡å åº¦è¿‡é«˜
    
    å‚æ•°:
        new_box: æ–°çš„çŸ©å½¢æ¡† (x1, y1, x2, y2)
        existing_boxes: å·²å­˜åœ¨çš„çŸ©å½¢æ¡†åˆ—è¡¨
        threshold: é‡å åº¦é˜ˆå€¼
    
    è¿”å›:
        Trueå¦‚æœé‡å åº¦è¶…è¿‡é˜ˆå€¼ï¼ŒFalseå¦åˆ™
    """
    for existing_box in existing_boxes:
        overlap_ratio = calculate_overlap_ratio(new_box, existing_box)
        if overlap_ratio > threshold:
            return True
    return False


def tile_image_with_overlap(img, tile_size, overlap_ratio=0.5, pad_value=0, geo_transform=None):
    """
    å°†å›¾åƒåˆ‡åˆ†ä¸ºå›ºå®šå¤§å°çš„å°å—ï¼Œä½¿ç”¨æŒ‡å®šçš„é‡å æ¯”ä¾‹ï¼Œä¸è¶³çš„åœ°æ–¹è¿›è¡Œå¡«å……

    å‚æ•°:
        img: PILå›¾åƒå¯¹è±¡
        tile_size: å°å—å¤§å° (width, height)
        overlap_ratio: é‡å æ¯”ä¾‹ (0-1ä¹‹é—´)
        pad_value: å¡«å……å€¼
        geo_transform: åœ°ç†åæ ‡å˜æ¢çŸ©é˜µ

    è¿”å›:
        tiles: åˆ‡åˆ†åçš„å°å—åˆ—è¡¨
        positions: æ¯ä¸ªå°å—åœ¨åŸå›¾ä¸­çš„ä½ç½® (x, y)
        boxes: æ¯ä¸ªå°å—çš„åœ°ç†åæ ‡è¾¹ç•Œæ¡†
    """
    width, height = img.size
    tile_width, tile_height = tile_size

    # è®¡ç®—æ­¥é•¿ï¼ˆéé‡å éƒ¨åˆ†çš„å¤§å°ï¼‰
    stride_w = int(tile_width * (1 - overlap_ratio))
    stride_h = int(tile_height * (1 - overlap_ratio))

    # ç¡®ä¿æ­¥é•¿è‡³å°‘ä¸º1
    stride_w = max(1, stride_w)
    stride_h = max(1, stride_h)

    # è®¡ç®—æ‰€éœ€çš„è¡Œåˆ—æ•°
    num_cols = math.ceil((width - tile_width) / stride_w) + 1 if width > tile_width else 1
    num_rows = math.ceil((height - tile_height) / stride_h) + 1 if height > tile_height else 1

    # è®¡ç®—éœ€è¦å¡«å……çš„å¤§å°
    pad_width = max(0, stride_w * (num_cols - 1) + tile_width - width)
    pad_height = max(0, stride_h * (num_rows - 1) + tile_height - height)

    # åˆ›å»ºå¡«å……åçš„å›¾åƒ
    padded_width = width + pad_width
    padded_height = height + pad_height

    if img.mode == 'L':
        padded_img = Image.new('L', (padded_width, padded_height), pad_value)
    else:  # RGBæˆ–å…¶ä»–æ¨¡å¼
        if isinstance(pad_value, int):
            pad_value = (pad_value,) * len(img.getbands())
        padded_img = Image.new(img.mode, (padded_width, padded_height), pad_value)

    # ç²˜è´´åŸå›¾
    padded_img.paste(img, (0, 0))

    tiles = []
    positions = []
    boxes = []

    # åˆ‡åˆ†å›¾åƒ
    for row in range(num_rows):
        for col in range(num_cols):
            x = col * stride_w
            y = row * stride_h

            # æå–å°å—
            pixel_box = (x, y, x + tile_width, y + tile_height)
            tile = padded_img.crop(pixel_box)

            # è½¬æ¢ä¸ºåœ°ç†åæ ‡è¾¹ç•Œæ¡†
            geo_box = pixel_box_to_geo_box(pixel_box, geo_transform)
            boxes.append(geo_box)

            tiles.append(tile)
            positions.append((x, y))

    return tiles, positions, boxes


def apply_geometric_augmentations(img_A, img_B, img_D, img_E, apply_h_flip, apply_v_flip, apply_rot90, apply_rot180, apply_rot270):
    """
    å¯¹å›¾åƒåº”ç”¨æŒ‡å®šçš„å‡ ä½•å˜æ¢æ•°æ®å¢å¼º

    å‚æ•°:
        img_A, img_B, img_D: è¾“å…¥å›¾åƒ
        img_E: æ ‡ç­¾å›¾åƒ
        apply_h_flip: æ˜¯å¦åº”ç”¨æ°´å¹³ç¿»è½¬
        apply_v_flip: æ˜¯å¦åº”ç”¨å‚ç›´ç¿»è½¬
        apply_rot90: æ˜¯å¦åº”ç”¨90Â°æ—‹è½¬
        apply_rot180: æ˜¯å¦åº”ç”¨180Â°æ—‹è½¬
        apply_rot270: æ˜¯å¦åº”ç”¨270Â°æ—‹è½¬

    è¿”å›:
        å¢å¼ºåçš„å›¾åƒåˆ—è¡¨
    """
    augmented_images = []

    # å§‹ç»ˆåŒ…æ‹¬åŸå§‹å›¾åƒ
    augmented_images.append((img_A, img_B, img_D, img_E, "original"))

    if apply_h_flip:
        aug_A = ImageOps.mirror(img_A)
        aug_B = ImageOps.mirror(img_B)
        aug_D = ImageOps.mirror(img_D)
        aug_E = ImageOps.mirror(img_E)
        augmented_images.append((aug_A, aug_B, aug_D, aug_E, "h_flip"))

    if apply_v_flip:
        aug_A = ImageOps.flip(img_A)
        aug_B = ImageOps.flip(img_B)
        aug_D = ImageOps.flip(img_D)
        aug_E = ImageOps.flip(img_E)
        augmented_images.append((aug_A, aug_B, aug_D, aug_E, "v_flip"))

    if apply_rot90:
        aug_A = img_A.rotate(90, expand=True)
        aug_B = img_B.rotate(90, expand=True)
        aug_D = img_D.rotate(90, expand=True)
        aug_E = img_E.rotate(90, expand=True)
        size = img_A.size
        aug_A = aug_A.resize(size)
        aug_B = aug_B.resize(size)
        aug_D = aug_D.resize(size)
        aug_E = aug_E.resize(size)
        augmented_images.append((aug_A, aug_B, aug_D, aug_E, "rot90"))

    if apply_rot180:
        aug_A = img_A.rotate(180)
        aug_B = img_B.rotate(180)
        aug_D = img_D.rotate(180)
        aug_E = img_E.rotate(180)
        augmented_images.append((aug_A, aug_B, aug_D, aug_E, "rot180"))

    if apply_rot270:
        aug_A = img_A.rotate(270, expand=True)
        aug_B = img_B.rotate(270, expand=True)
        aug_D = img_D.rotate(270, expand=True)
        aug_E = img_E.rotate(270, expand=True)
        size = img_A.size
        aug_A = aug_A.resize(size)
        aug_B = aug_B.resize(size)
        aug_D = aug_D.resize(size)
        aug_E = aug_E.resize(size)
        augmented_images.append((aug_A, aug_B, aug_D, aug_E, "rot270"))

    return augmented_images


def is_acceptable_size_difference(sizes, tolerance=2):
    """
    æ£€æŸ¥å°ºå¯¸å·®å¼‚æ˜¯å¦åœ¨å¯æ¥å—èŒƒå›´å†…

    å‚æ•°:
        sizes: å°ºå¯¸åˆ—è¡¨
        tolerance: å…è®¸çš„åƒç´ å·®å¼‚é˜ˆå€¼

    è¿”å›:
        æ˜¯å¦å¯æ¥å—
    """
    max_width = max(w for w, h in sizes)
    min_width = min(w for w, h in sizes)
    max_height = max(h for w, h in sizes)
    min_height = min(h for w, h in sizes)

    width_diff = max_width - min_width
    height_diff = max_height - min_height

    return width_diff <= tolerance and height_diff <= tolerance


# ========================= æ–°å¢ï¼šæµå¼å…ˆåˆ‡ç‰‡å†åˆ’åˆ† ========================= #

def crop_with_padding(img, pixel_box, tile_size, pad_value=0):
    """
    ä»å›¾åƒè£å‰ª pixel_box æ‰€ç¤ºåŒºåŸŸï¼Œä¸è¶³å¤„ç”¨ pad_value å¡«å……è‡³ tile_size å¤§å°ã€‚
    """
    x1, y1, x2, y2 = pixel_box
    tile_w, tile_h = tile_size
    img_w, img_h = img.size

    crop_box = (
        max(0, x1),
        max(0, y1),
        min(x2, img_w),
        min(y2, img_h)
    )
    region = img.crop(crop_box)

    if img.mode == 'L':
        canvas = Image.new('L', (tile_w, tile_h), pad_value)
    else:
        pv = pad_value if isinstance(pad_value, tuple) else (pad_value,) * len(img.getbands())
        canvas = Image.new(img.mode, (tile_w, tile_h), pv)

    canvas.paste(region, (0, 0))
    return canvas


def decide_split_for_tile_with_global(
    val_count, train_count,
    fg_val_sum, pix_val_sum,
    global_fg_ratio,
    val_ratio,
    tile_fg, tile_pix
):
    """
    ä½¿ç”¨å›ºå®šçš„å…¨å±€å‰æ™¯æ¯”ä¾‹(global_fg_ratio)è¿›è¡Œåœ¨çº¿è´ªå¿ƒå†³ç­–ã€‚
    ç›®æ ‡ï¼šåŒæ—¶é€¼è¿‘æ•°é‡æ¯”ä¾‹(val_ratio)ä¸å‰æ™¯æ¯”ä¾‹(global_fg_ratio)ã€‚
    è¿”å› 'train' æˆ– 'val'ã€‚
    """
    total_so_far = val_count + train_count
    # æ–¹æ¡ˆä¸€ï¼šæ”¾å…¥val
    val_count_1 = val_count + 1
    val_fraction_1 = val_count_1 / (total_so_far + 1)
    fg_val_1 = fg_val_sum + tile_fg
    pix_val_1 = pix_val_sum + tile_pix
    val_fg_ratio_1 = (fg_val_1 / pix_val_1) if pix_val_1 > 0 else 0.0
    obj_1 = abs(val_fraction_1 - val_ratio) + abs(val_fg_ratio_1 - global_fg_ratio)

    # æ–¹æ¡ˆäºŒï¼šæ”¾å…¥train
    val_fraction_2 = val_count / (total_so_far + 1)
    val_fg_ratio_2 = (fg_val_sum / pix_val_sum) if pix_val_sum > 0 else 0.0
    obj_2 = abs(val_fraction_2 - val_ratio) + abs(val_fg_ratio_2 - global_fg_ratio)

    if obj_1 < obj_2:
        return 'val'
    elif obj_2 < obj_1:
        return 'train'
    else:
        return 'val' if val_fraction_1 < val_ratio else 'train'


def process_and_split_dataset_streaming(
    input_dir, output_dir, tile_size=(256, 256), overlap_ratio=0.5,
    size_tolerance=2, val_ratio=0.2, create_test_folder=True,
    overlap_threshold=0.8, filter_black_tiles=True, black_threshold=0.95,
    seed=666
):
    """
    æµå¼å¤„ç†ï¼ˆæ”¹ä¸ºä¸¤é˜¶æ®µä½†ä¸è½ç›˜æš‚å­˜ï¼‰ï¼š
    1) æ”¶é›†é˜¶æ®µï¼šéå†æ‰€æœ‰åŸå›¾ä¸ä½ç½®ï¼Œåšå»é‡ä¸é»‘å—è¿‡æ»¤ï¼Œè®¡ç®—å‰æ™¯åƒç´ å¹¶è®°å½•tileå…ƒæ•°æ®ï¼›ä¸ä¿å­˜åƒç´ ã€‚
    2) åˆ’åˆ†ä¿å­˜ï¼šå¯¹æ”¶é›†åˆ°çš„tileéšæœºæ‰“ä¹±ï¼ˆseed=666ï¼‰ï¼ŒæŒ‰æ•°é‡+å‰æ™¯æ¯”ä¾‹è´ªå¿ƒåˆ’åˆ†ï¼Œå¹¶ä¸€æ¬¡æ€§è£å‰ªä¿å­˜åˆ°train/valï¼›valå¤åˆ¶åˆ°testã€‚
    """
    random.seed(seed)
    np.random.seed(seed)

    base_names = find_base_names_from_folder(input_dir)
    if not base_names:
        print(f"åœ¨ {input_dir} ä¸­æœªæ‰¾åˆ°ç¬¦åˆæ ¼å¼çš„å›¾åƒ")
        return

    print(f"æ‰¾åˆ° {len(base_names)} ç»„åŸå§‹å›¾åƒï¼ˆæµå¼ï¼šæ”¶é›†â†’æ‰“ä¹±â†’åˆ’åˆ†â†’ä¿å­˜ï¼‰")

    train_folders, val_folders, test_folders = create_dataset_folders(output_dir, create_test_folder)

    # æ”¶é›†é˜¶æ®µ
    collected_tiles = []  # æ¯é¡¹: {base, x, y, pixel_box, geo_box, tile_fg, tile_pix}
    global_boxes = []     # å»é‡ç›’ï¼ˆåœ°ç†ï¼‰

    total_generated_tiles = 0
    total_filtered_overlap_tiles = 0
    total_filtered_black_tiles = 0

    print("é˜¶æ®µ1ï¼šæ”¶é›†å€™é€‰tileï¼ˆå»é‡ä¸é»‘å—è¿‡æ»¤ï¼‰...")

    for base_name in tqdm(base_names, desc="æ”¶é›†åŸå›¾"):
        path_A = os.path.join(input_dir, f"{base_name}_A.tif")
        path_B = os.path.join(input_dir, f"{base_name}_B.tif")
        path_D = os.path.join(input_dir, f"{base_name}_D.tif")
        path_E = os.path.join(input_dir, f"{base_name}_E.png")

        if not all(os.path.exists(p) for p in [path_A, path_B, path_D, path_E]):
            print(f"è­¦å‘Š: æ–‡ä»¶é›† {base_name} ä¸å®Œæ•´ï¼Œè·³è¿‡")
            continue

        try:
            img_A = Image.open(path_A)
            img_B = Image.open(path_B)
            img_D = Image.open(path_D)
            img_E = Image.open(path_E).convert('L')
            geo_transform = get_geo_transform(path_A)
        except Exception as e:
            print(f"è­¦å‘Š: æ‰“å¼€æ–‡ä»¶é›† {base_name} æ—¶å‡ºé”™: {e}")
            continue

        sizes = [img_A.size, img_B.size, img_D.size, img_E.size]
        if len(set(sizes)) > 1:
            if is_acceptable_size_difference(sizes, size_tolerance):
                min_w = min(w for w, h in sizes)
                min_h = min(h for w, h in sizes)
                if img_A.size != (min_w, min_h):
                    img_A = img_A.crop((0, 0, min_w, min_h))
                if img_B.size != (min_w, min_h):
                    img_B = img_B.crop((0, 0, min_w, min_h))
                if img_D.size != (min_w, min_h):
                    img_D = img_D.crop((0, 0, min_w, min_h))
                if img_E.size != (min_w, min_h):
                    img_E = img_E.crop((0, 0, min_w, min_h))
                print(f"ä¿¡æ¯: æ–‡ä»¶é›† {base_name} å°ºå¯¸å·²è°ƒæ•´ä¸º {min_w}x{min_h}")
            else:
                print(f"è­¦å‘Š: æ–‡ä»¶é›† {base_name} å°ºå¯¸å·®å¼‚è¿‡å¤§ {sizes}ï¼Œè·³è¿‡")
                continue

        width, height = img_A.size
        tile_w, tile_h = tile_size
        stride_w = max(1, int(tile_w * (1 - overlap_ratio)))
        stride_h = max(1, int(tile_h * (1 - overlap_ratio)))
        num_cols = math.ceil((width - tile_w) / stride_w) + 1 if width > tile_w else 1
        num_rows = math.ceil((height - tile_h) / stride_h) + 1 if height > tile_h else 1

        for row in range(num_rows):
            for col in range(num_cols):
                x = col * stride_w
                y = row * stride_h
                pixel_box = (x, y, x + tile_w, y + tile_h)
                geo_box = pixel_box_to_geo_box(pixel_box, geo_transform)

                total_generated_tiles += 1

                # å»é‡åœ¨è£å‰ªå‰
                if check_overlap_with_existing(geo_box, global_boxes, overlap_threshold):
                    total_filtered_overlap_tiles += 1
                    continue

                # ç”¨Aåˆ¤æ–­é»‘å—
                tile_A_small = crop_with_padding(img_A, pixel_box, tile_size, pad_value=0)
                if filter_black_tiles and is_black_tile(tile_A_small, black_threshold):
                    total_filtered_black_tiles += 1
                    continue

                # è®¡ç®—å‰æ™¯åƒç´ ï¼ˆEï¼‰
                tile_E_small = crop_with_padding(img_E, pixel_box, tile_size, pad_value=0)
                arr_e = np.array(tile_E_small)
                tile_fg = int((arr_e > 0).sum())
                tile_pix = arr_e.shape[0] * arr_e.shape[1]

                collected_tiles.append({
                    'base': base_name,
                    'x': x,
                    'y': y,
                    'pixel_box': pixel_box,
                    'geo_box': geo_box,
                    'tile_fg': tile_fg,
                    'tile_pix': tile_pix
                })
                global_boxes.append(geo_box)

    if not collected_tiles:
        print("æ— å¯ç”¨tileï¼Œç»“æŸã€‚")
        return

    # é¢„è®¡ç®—å…¨å±€å‰æ™¯æ¯”ä¾‹ï¼ˆå›ºå®šç›®æ ‡ï¼‰
    global_fg = sum(t['tile_fg'] for t in collected_tiles)
    global_pix = sum(t['tile_pix'] for t in collected_tiles)
    global_fg_ratio = (global_fg / global_pix) if global_pix > 0 else 0.0

    # æ‰“ä¹±ï¼ˆå›ºå®šç§å­ï¼‰
    rng = np.random.RandomState(seed)
    rng.shuffle(collected_tiles)

    # åˆ’åˆ† + ä¿å­˜é˜¶æ®µ
    print("é˜¶æ®µ2ï¼šéšæœºæ‰“ä¹±ååˆ’åˆ†ä¸ä¿å­˜...")

    val_count = 0
    train_count = 0
    fg_val_sum = 0
    pix_val_sum = 0
    fg_train_sum = 0
    pix_train_sum = 0

    total_saved_tiles = 0

    # ä¸ºè£å‰ªä¿å­˜ï¼ŒæŒ‰åŸå›¾åˆ†ç»„æ‰“å¼€ï¼Œé¿å…é¢‘ç¹æ‰“å¼€å…³é—­
    # ç®€åŒ–å®ç°ï¼šé€tileæŒ‰éœ€æ‰“å¼€ï¼ˆä¿æŒå¯è¯»æ€§ï¼‰

    for t in tqdm(collected_tiles, desc="ä¿å­˜tile"):
        base_name = t['base']
        x = t['x']
        y = t['y']
        pixel_box = t['pixel_box']
        tile_fg = t['tile_fg']
        tile_pix = t['tile_pix']

        split_assignment = decide_split_for_tile_with_global(
            val_count, train_count,
            fg_val_sum, pix_val_sum,
            global_fg_ratio,
            val_ratio,
            tile_fg, tile_pix
        )

        new_base_name = f"{base_name}_original_x{x}_y{y}"

        # æ‰“å¼€å›¾åƒå¹¶å®é™…è£å‰ªã€ä¿å­˜
        path_A = os.path.join(input_dir, f"{base_name}_A.tif")
        path_B = os.path.join(input_dir, f"{base_name}_B.tif")
        path_D = os.path.join(input_dir, f"{base_name}_D.tif")
        path_E = os.path.join(input_dir, f"{base_name}_E.png")
        try:
            img_A = Image.open(path_A)
            img_B = Image.open(path_B)
            img_D = Image.open(path_D)
            img_E = Image.open(path_E).convert('L')
        except Exception as e:
            print(f"è­¦å‘Š: æ‰“å¼€æ–‡ä»¶é›† {base_name} æ—¶å‡ºé”™: {e}")
            continue

        tile_A_small = crop_with_padding(img_A, pixel_box, tile_size, pad_value=0)
        tile_B_small = crop_with_padding(img_B, pixel_box, tile_size, pad_value=0)
        tile_D_small = crop_with_padding(img_D, pixel_box, tile_size, pad_value=0)
        tile_E_small = crop_with_padding(img_E, pixel_box, tile_size, pad_value=0)

        target = train_folders if split_assignment == 'train' else val_folders
        try:
            tile_A_small.save(os.path.join(target['A'], f"{new_base_name}.png"), "PNG")
            tile_B_small.save(os.path.join(target['B'], f"{new_base_name}.png"), "PNG")
            tile_D_small.save(os.path.join(target['C'], f"{new_base_name}.png"), "PNG")
            tile_E_small.save(os.path.join(target['label'], f"{new_base_name}.png"), "PNG")

            if split_assignment == 'val' and create_test_folder and test_folders:
                tile_A_small.save(os.path.join(test_folders['A'], f"{new_base_name}.png"), "PNG")
                tile_B_small.save(os.path.join(test_folders['B'], f"{new_base_name}.png"), "PNG")
                tile_D_small.save(os.path.join(test_folders['C'], f"{new_base_name}.png"), "PNG")
                tile_E_small.save(os.path.join(test_folders['label'], f"{new_base_name}.png"), "PNG")

            total_saved_tiles += 1

            if split_assignment == 'val':
                val_count += 1
                fg_val_sum += tile_fg
                pix_val_sum += tile_pix
            else:
                train_count += 1
                fg_train_sum += tile_fg
                pix_train_sum += tile_pix
        except Exception as e:
            print(f"ä¿å­˜å°å—æ—¶å‡ºé”™: {e}")

    print("\nå¤„ç†å®Œæˆï¼ˆæµå¼ï¼‰ï¼")
    print(f"è®­ç»ƒé›†: {train_count} ä¸ªå°å—")
    print(f"éªŒè¯é›†: {val_count} ä¸ªå°å—")
    if create_test_folder:
        print(f"æµ‹è¯•é›†: {val_count} ä¸ªå°å— (ä¸éªŒè¯é›†ç›¸åŒ)")
    print(f"æ€»å…±ç”Ÿæˆ {total_generated_tiles} ä¸ªå°å—ï¼Œä¿å­˜ {total_saved_tiles} ä¸ªå°å—")
    print(f"é‡å åº¦é˜ˆå€¼: {overlap_threshold * 100:.1f}%")
    print(f"æ€»å…±è¿‡æ»¤äº† {total_filtered_overlap_tiles} ä¸ªé‡å å°å— å’Œ {total_filtered_black_tiles} ä¸ªçº¯é»‘è‰²å°å—")


def create_dataset_folders(output_dir, create_test_folder=True):
    """
    åˆ›å»ºæ•°æ®é›†æ–‡ä»¶å¤¹ç»“æ„
    
    å‚æ•°:
        output_dir: è¾“å‡ºæ ¹ç›®å½•
        create_test_folder: æ˜¯å¦åˆ›å»ºæµ‹è¯•é›†æ–‡ä»¶å¤¹
    
    è¿”å›:
        train_folders, val_folders, test_folders
    """
    # åˆ›å»ºä¸»ç›®å½•ç»“æ„
    train_folder = os.path.join(output_dir, "train")
    val_folder = os.path.join(output_dir, "val")
    test_folder = os.path.join(output_dir, "test") if create_test_folder else None

    # è®­ç»ƒé›†æ–‡ä»¶å¤¹
    train_folders = {
        'A': os.path.join(train_folder, "A"),
        'B': os.path.join(train_folder, "B"),
        'C': os.path.join(train_folder, "C"),
        'label': os.path.join(train_folder, "label")
    }

    # éªŒè¯é›†æ–‡ä»¶å¤¹
    val_folders = {
        'A': os.path.join(val_folder, "A"),
        'B': os.path.join(val_folder, "B"),
        'C': os.path.join(val_folder, "C"),
        'label': os.path.join(val_folder, "label")
    }

    # æµ‹è¯•é›†æ–‡ä»¶å¤¹
    test_folders = None
    if create_test_folder:
        test_folders = {
            'A': os.path.join(test_folder, "A"),
            'B': os.path.join(test_folder, "B"),
            'C': os.path.join(test_folder, "C"),
            'label': os.path.join(test_folder, "label")
        }

    # åˆ›å»ºæ‰€æœ‰æ–‡ä»¶å¤¹
    all_folders = list(train_folders.values()) + list(val_folders.values())
    if test_folders:
        all_folders.extend(list(test_folders.values()))

    for folder in all_folders:
        os.makedirs(folder, exist_ok=True)

    return train_folders, val_folders, test_folders


def find_base_names_from_folder(input_dir):
    """
    ä»æ–‡ä»¶å¤¹ä¸­æ‰¾å‡ºæ‰€æœ‰åŸºç¡€åç§°

    å‚æ•°:
        input_dir: è¾“å…¥ç›®å½•

    è¿”å›:
        base_names: åŸºç¡€åç§°åˆ—è¡¨
    """
    base_names = set()

    # æŸ¥æ‰¾æ‰€æœ‰Aç±»å‹æ–‡ä»¶
    a_files = glob.glob(os.path.join(input_dir, "*_A.tif"))
    for a_file in a_files:
        # å»æ‰è·¯å¾„å’Œåç¼€
        filename = os.path.basename(a_file)
        # å»æ‰_A.tiféƒ¨åˆ†
        base_name = filename[:-6] if filename.endswith("_A.tif") else filename
        base_names.add(base_name)

    return list(base_names)


def process_image_set_with_overlap_filter(base_name, input_dir, train_folders, val_folders, test_folders,
                                        tile_size=(256, 256), overlap_ratio=0.5, pad_value=0,
                                        size_tolerance=2, apply_augmentation=True,
                                        apply_h_flip=APPLY_H_FLIP, apply_v_flip=APPLY_V_FLIP,
                                        apply_rot90=APPLY_ROT90, apply_rot180=APPLY_ROT180, apply_rot270=APPLY_ROT270,
                                        overlap_threshold=0.8, split_assignment="train", global_boxes=None,
                                        filter_black_tiles=True, black_threshold=0.95):
    """
    å¤„ç†ä¸€ç»„ç›¸å…³çš„å›¾åƒ(Aã€Bã€Dã€E)ï¼Œåˆ‡åˆ†ä¸ºé‡å çš„å°å—ï¼Œè¿‡æ»¤é«˜é‡å åº¦çš„å°å—ï¼Œå¹¶ç›´æ¥ä¿å­˜åˆ°æŒ‡å®šæ•°æ®é›†

    å‚æ•°:
        base_name: å›¾åƒåŸºç¡€åç§°
        input_dir: è¾“å…¥ç›®å½•
        train_folders: è®­ç»ƒé›†æ–‡ä»¶å¤¹å­—å…¸
        val_folders: éªŒè¯é›†æ–‡ä»¶å¤¹å­—å…¸
        test_folders: æµ‹è¯•é›†æ–‡ä»¶å¤¹å­—å…¸ï¼ˆå¯é€‰ï¼‰
        tile_size: å°å—å¤§å° (width, height)
        overlap_ratio: é‡å æ¯”ä¾‹
        pad_value: å¡«å……å€¼
        size_tolerance: å…è®¸çš„å°ºå¯¸å·®å¼‚åƒç´ æ•°
        apply_augmentation: æ˜¯å¦åº”ç”¨æ•°æ®å¢å¼º
        apply_h_flip: æ˜¯å¦åº”ç”¨æ°´å¹³ç¿»è½¬
        apply_v_flip: æ˜¯å¦åº”ç”¨å‚ç›´ç¿»è½¬
        apply_rot90: æ˜¯å¦åº”ç”¨90Â°æ—‹è½¬
        apply_rot180: æ˜¯å¦åº”ç”¨180Â°æ—‹è½¬
        apply_rot270: æ˜¯å¦åº”ç”¨270Â°æ—‹è½¬
        overlap_threshold: é‡å åº¦é˜ˆå€¼
        split_assignment: æ•°æ®é›†åˆ†é… ("train" æˆ– "val")
        global_boxes: å…¨å±€è¾¹ç•Œæ¡†åˆ—è¡¨ï¼Œç”¨äºè·¨å›¾åƒçš„é‡å æ£€æµ‹
        filter_black_tiles: æ˜¯å¦è¿‡æ»¤çº¯é»‘è‰²å°å—
        black_threshold: çº¯é»‘è‰²åˆ¤å®šé˜ˆå€¼

    è¿”å›:
        (ä¿å­˜çš„å°å—æ•°é‡, ç”Ÿæˆçš„å°å—æ•°é‡)
    """
    # æ„å»ºæ–‡ä»¶è·¯å¾„
    path_A = os.path.join(input_dir, f"{base_name}_A.tif")
    path_B = os.path.join(input_dir, f"{base_name}_B.tif")
    path_D = os.path.join(input_dir, f"{base_name}_D.tif")
    path_E = os.path.join(input_dir, f"{base_name}_E.png")

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not all(os.path.exists(p) for p in [path_A, path_B, path_D, path_E]):
        print(f"è­¦å‘Š: æ–‡ä»¶é›† {base_name} ä¸å®Œæ•´ï¼Œè·³è¿‡")
        return None

    # è¯»å–å›¾åƒ
    try:
        img_A = Image.open(path_A)
        img_B = Image.open(path_B)
        img_D = Image.open(path_D)
        img_E = Image.open(path_E).convert('L')  # ç¡®ä¿æ ‡ç­¾æ˜¯ç°åº¦å›¾
        
        # è·å–åœ°ç†åæ ‡å˜æ¢ä¿¡æ¯ï¼ˆä»¥Aå›¾åƒä¸ºå‡†ï¼‰
        geo_transform = get_geo_transform(path_A)
        print(f"ä¿¡æ¯: æ–‡ä»¶é›† {base_name} ä½¿ç”¨åœ°ç†åæ ‡è¿›è¡Œé‡å æ£€æµ‹")
            
    except Exception as e:
        print(f"è­¦å‘Š: æ‰“å¼€æ–‡ä»¶é›† {base_name} æ—¶å‡ºé”™: {e}")
        return None

    # æ£€æŸ¥å°ºå¯¸ä¸€è‡´æ€§
    sizes = [img_A.size, img_B.size, img_D.size, img_E.size]

    # å¦‚æœå°ºå¯¸ä¸å®Œå…¨ä¸€è‡´ï¼Œä½†å·®å¼‚åœ¨å¯æ¥å—èŒƒå›´å†…ï¼Œåˆ™è°ƒæ•´å°ºå¯¸
    if len(set(sizes)) > 1:
        if is_acceptable_size_difference(sizes, size_tolerance):
            # æ‰¾å‡ºæœ€å°çš„å…±åŒå°ºå¯¸
            min_width = min(w for w, h in sizes)
            min_height = min(h for w, h in sizes)

            # è°ƒæ•´æ‰€æœ‰å›¾åƒåˆ°ç›¸åŒå°ºå¯¸
            if img_A.size != (min_width, min_height):
                img_A = img_A.crop((0, 0, min_width, min_height))
            if img_B.size != (min_width, min_height):
                img_B = img_B.crop((0, 0, min_width, min_height))
            if img_D.size != (min_width, min_height):
                img_D = img_D.crop((0, 0, min_width, min_height))
            if img_E.size != (min_width, min_height):
                img_E = img_E.crop((0, 0, min_width, min_height))

            print(f"ä¿¡æ¯: æ–‡ä»¶é›† {base_name} å°ºå¯¸å·²è°ƒæ•´ä¸º {min_width}x{min_height}")
        else:
            print(f"è­¦å‘Š: æ–‡ä»¶é›† {base_name} å°ºå¯¸å·®å¼‚è¿‡å¤§ {sizes}ï¼Œè·³è¿‡")
            return None

    total_tiles = 0
    saved_tiles = 0
    filtered_overlap_tiles = 0
    filtered_black_tiles = 0

    # åº”ç”¨æ•°æ®å¢å¼º
    if apply_augmentation:
        augmented_images = apply_geometric_augmentations(img_A, img_B, img_D, img_E, apply_h_flip, apply_v_flip, apply_rot90, apply_rot180, apply_rot270)
    else:
        augmented_images = [(img_A, img_B, img_D, img_E, "original")]

    # é€‰æ‹©ç›®æ ‡æ–‡ä»¶å¤¹
    if split_assignment == "train":
        target_folders = train_folders
    else:
        target_folders = val_folders

    # å¯¹æ¯ä¸ªå¢å¼ºåçš„å›¾åƒé›†åˆè¿›è¡Œé‡å å¼åˆ‡åˆ†å’Œä¿å­˜
    for aug_A, aug_B, aug_D, aug_E, aug_type in augmented_images:
        # é‡å å¼åˆ‡åˆ†å›¾åƒä¸ºå°å—
        tiles_A, positions, boxes = tile_image_with_overlap(aug_A, tile_size, overlap_ratio, geo_transform=geo_transform)
        tiles_B, _, _ = tile_image_with_overlap(aug_B, tile_size, overlap_ratio, geo_transform=geo_transform)
        tiles_D, _, _ = tile_image_with_overlap(aug_D, tile_size, overlap_ratio, geo_transform=geo_transform)
        tiles_E, _, _ = tile_image_with_overlap(aug_E, tile_size, overlap_ratio, pad_value=0, geo_transform=geo_transform)  # æ ‡ç­¾ç”¨0å¡«å……

        # ä¿å­˜åˆ‡åˆ†åçš„å°å—
        for i, ((x, y), box, tile_A, tile_B, tile_D, tile_E) in enumerate(
                zip(positions, boxes, tiles_A, tiles_B, tiles_D, tiles_E)):
            
            total_tiles += 1
            
            # æ£€æŸ¥æ˜¯å¦ä¸å·²ä¿å­˜çš„å°å—é‡å åº¦è¿‡é«˜
            if global_boxes is not None and check_overlap_with_existing(box, global_boxes, overlap_threshold):
                filtered_overlap_tiles += 1
                continue  # è·³è¿‡é‡å åº¦è¿‡é«˜çš„å°å—
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯é»‘è‰²å°å—
            if filter_black_tiles and is_black_tile(tile_A, black_threshold):
                filtered_black_tiles += 1
                continue  # è·³è¿‡çº¯é»‘è‰²å°å—
            
            # æ„å»ºæ–°çš„åŸºç¡€åç§°ï¼ŒåŒ…å«åŸå§‹åæ ‡ä¿¡æ¯å’Œå¢å¼ºç±»å‹
            new_base_name = f"{base_name}_{aug_type}_x{x}_y{y}"

            try:
                # ä¿å­˜åˆ°ç›®æ ‡æ•°æ®é›†
                # Aç±»å›¾åƒ
                tile_A.save(os.path.join(target_folders['A'], f"{new_base_name}.png"), "PNG")
                # Bç±»å›¾åƒ
                tile_B.save(os.path.join(target_folders['B'], f"{new_base_name}.png"), "PNG")
                # Dç±»å›¾åƒï¼ˆä¿å­˜ä¸ºCï¼‰
                tile_D.save(os.path.join(target_folders['C'], f"{new_base_name}.png"), "PNG")
                # æ ‡ç­¾å›¾åƒ
                tile_E.save(os.path.join(target_folders['label'], f"{new_base_name}.png"), "PNG")

                # å¦‚æœæ˜¯éªŒè¯é›†ä¸”éœ€è¦åˆ›å»ºæµ‹è¯•é›†ï¼Œä¹Ÿä¿å­˜åˆ°æµ‹è¯•é›†
                if split_assignment == "val" and test_folders:
                    tile_A.save(os.path.join(test_folders['A'], f"{new_base_name}.png"), "PNG")
                    tile_B.save(os.path.join(test_folders['B'], f"{new_base_name}.png"), "PNG")
                    tile_D.save(os.path.join(test_folders['C'], f"{new_base_name}.png"), "PNG")
                    tile_E.save(os.path.join(test_folders['label'], f"{new_base_name}.png"), "PNG")

                saved_tiles += 1
                
                # å°†å·²ä¿å­˜çš„è¾¹ç•Œæ¡†æ·»åŠ åˆ°å…¨å±€åˆ—è¡¨ä¸­
                if global_boxes is not None:
                    global_boxes.append(box)
                
            except Exception as e:
                print(f"ä¿å­˜å°å—æ—¶å‡ºé”™: {e}")

    if total_tiles > saved_tiles:
        filter_info = []
        if filtered_overlap_tiles > 0:
            filter_info.append(f"{filtered_overlap_tiles} ä¸ªé‡å å°å—")
        if filtered_black_tiles > 0:
            filter_info.append(f"{filtered_black_tiles} ä¸ªçº¯é»‘è‰²å°å—")
        
        if filter_info:
            filter_text = "è¿‡æ»¤äº† " + " å’Œ ".join(filter_info)
        else:
            filter_text = f"è¿‡æ»¤äº† {total_tiles - saved_tiles} ä¸ªå°å—"
            
        print(f"æ–‡ä»¶é›† {base_name}: ç”Ÿæˆ {total_tiles} ä¸ªå°å—ï¼Œä¿å­˜ {saved_tiles} ä¸ªå°å— ({filter_text})")
    
    return (saved_tiles, total_tiles)


def process_and_split_dataset(input_dir, output_dir, tile_size=(256, 256), overlap_ratio=0.5, 
                            size_tolerance=2, val_ratio=0.2, create_test_folder=True,
                            apply_augmentation=True, apply_h_flip=APPLY_H_FLIP, apply_v_flip=APPLY_V_FLIP,
                            apply_rot90=APPLY_ROT90, apply_rot180=APPLY_ROT180, apply_rot270=APPLY_ROT270,
                            overlap_threshold=0.8, filter_black_tiles=True, black_threshold=0.95,
                            geo_aware=True, geo_eps=2000, geo_min_samples=1):
    """
    å¤„ç†æ•´ä¸ªæ•°æ®é›†çš„å›¾åƒï¼Œåº”ç”¨é‡å è¿‡æ»¤ï¼Œå¹¶ç›´æ¥åˆ†å‰²ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†

    å‚æ•°:
        input_dir: è¾“å…¥ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        tile_size: å°å—å¤§å° (width, height)
        overlap_ratio: é‡å æ¯”ä¾‹
        size_tolerance: å…è®¸çš„å°ºå¯¸å·®å¼‚åƒç´ æ•°
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        create_test_folder: æ˜¯å¦åˆ›å»ºæµ‹è¯•é›†æ–‡ä»¶å¤¹
        apply_augmentation: æ˜¯å¦åº”ç”¨æ•°æ®å¢å¼º
        apply_h_flip: æ˜¯å¦åº”ç”¨æ°´å¹³ç¿»è½¬
        apply_v_flip: æ˜¯å¦åº”ç”¨å‚ç›´ç¿»è½¬
        apply_rot90: æ˜¯å¦åº”ç”¨90Â°æ—‹è½¬
        apply_rot180: æ˜¯å¦åº”ç”¨180Â°æ—‹è½¬
        apply_rot270: æ˜¯å¦åº”ç”¨270Â°æ—‹è½¬
        overlap_threshold: é‡å åº¦é˜ˆå€¼
        filter_black_tiles: æ˜¯å¦è¿‡æ»¤çº¯é»‘è‰²å°å—
        black_threshold: çº¯é»‘è‰²åˆ¤å®šé˜ˆå€¼
        geo_aware: æ˜¯å¦å¯ç”¨åœ°ç†æ„ŸçŸ¥åˆ’åˆ†
        geo_eps: åœ°ç†èšç±»é‚»åŸŸåŠå¾„ï¼ˆç±³ï¼‰
        geo_min_samples: åœ°ç†èšç±»æœ€å°æ ·æœ¬æ•°
    """
    # è·å–æ‰€æœ‰åŸºç¡€åç§°
    base_names = find_base_names_from_folder(input_dir)

    if not base_names:
        print(f"åœ¨ {input_dir} ä¸­æœªæ‰¾åˆ°ç¬¦åˆæ ¼å¼çš„å›¾åƒ")
        return

    print(f"æ‰¾åˆ° {len(base_names)} ç»„åŸå§‹å›¾åƒ")

    # åˆ›å»ºæ•°æ®é›†æ–‡ä»¶å¤¹ç»“æ„
    train_folders, val_folders, test_folders = create_dataset_folders(output_dir, create_test_folder)

    # é¢„è®¡ç®—æ‰€æœ‰åŸºç¡€å›¾çš„å‰æ™¯åƒç´ ç»Ÿè®¡
    print("ç»Ÿè®¡æ•´å›¾å‰æ™¯åƒç´ æ¯”ä¾‹(ç”¨äºå‡è¡¡åˆ’åˆ†)...")
    fg_stats = compute_fg_stats_for_basenames(input_dir, base_names)

    # ğŸŒ åœ°ç†æ„ŸçŸ¥çš„æ•°æ®é›†åˆ’åˆ†
    if geo_aware and SKLEARN_AVAILABLE:
        print("ä½¿ç”¨åœ°ç†æ„ŸçŸ¥åˆ’åˆ†æ¨¡å¼...")

        # åˆ†æåœ°ç†åæ ‡åˆ†å¸ƒ
        coords, valid_names = analyze_geographic_distribution(input_dir, base_names)

        if len(valid_names) < len(base_names):
            print(f"è­¦å‘Š: {len(base_names) - len(valid_names)} ä¸ªå›¾åƒæ— æ³•è¯»å–åœ°ç†åæ ‡ï¼Œå·²è·³è¿‡")
            base_names = valid_names

        if len(coords) < 2:
            print("è­¦å‘Š: å¯ç”¨å›¾åƒæ•°é‡ä¸è¶³ï¼Œå›é€€åˆ°éšæœºåˆ’åˆ†")
            random.shuffle(base_names)
            split_idx = int(len(base_names) * (1 - val_ratio))
            train_base_names = base_names[:split_idx]
            val_base_names = base_names[split_idx:]
        else:
            # æ‰§è¡Œåœ°ç†èšç±»
            cluster_labels = perform_geographic_clustering(coords, geo_eps, geo_min_samples)

            # åŸºäºèšç±»è¿›è¡Œè®­ç»ƒ/éªŒè¯é›†åˆ’åˆ†ï¼ˆå‡è¡¡å‰æ™¯æ¯”ä¾‹ï¼‰
            train_base_names, val_base_names = geo_aware_train_val_split_balanced(
                base_names, cluster_labels, val_ratio, fg_stats
            )

            # éªŒè¯åœ°ç†åˆ†ç¦»ç¨‹åº¦
            verify_geographic_separation(train_base_names, val_base_names, input_dir)
    else:
        # ä¼ ç»Ÿéšæœºåˆ’åˆ†ï¼ˆå‘åå…¼å®¹ï¼‰
        if not geo_aware:
            print("ä½¿ç”¨ä¼ ç»Ÿéšæœºåˆ’åˆ†æ¨¡å¼...")
        else:
            print("sklearnä¸å¯ç”¨ï¼Œå›é€€åˆ°éšæœºåˆ’åˆ†æ¨¡å¼...")

        random.shuffle(base_names)
        split_idx = int(len(base_names) * (1 - val_ratio))
        train_base_names = base_names[:split_idx]
        val_base_names = base_names[split_idx:]

    print(f"è®­ç»ƒé›†: {len(train_base_names)} ç»„å›¾åƒ")
    print(f"éªŒè¯é›†: {len(val_base_names)} ç»„å›¾åƒ")

    # è¾“å‡ºåˆ’åˆ†çš„å‰æ™¯æ¯”ä¾‹ç»Ÿè®¡
    print_split_fg_summary("è®­ç»ƒé›†(æ•´å›¾)", train_base_names, fg_stats)
    print_split_fg_summary("éªŒè¯é›†(æ•´å›¾)", val_base_names, fg_stats)

    # åˆ†ç¦»çš„è¾¹ç•Œæ¡†åˆ—è¡¨ï¼Œé¿å…è®­ç»ƒé›†å’ŒéªŒè¯é›†ç›¸äº’è¿‡æ»¤
    train_global_boxes = []  # è®­ç»ƒé›†å†…éƒ¨é‡å æ£€æµ‹
    val_global_boxes = []    # éªŒè¯é›†å†…éƒ¨é‡å æ£€æµ‹

    # å¤„ç†è®­ç»ƒé›†
    total_train_tiles = 0
    total_generated_train_tiles = 0
    processed_train_groups = 0

    print("å¤„ç†è®­ç»ƒé›†...")
    for base_name in tqdm(train_base_names, desc="å¤„ç†è®­ç»ƒé›†å›¾åƒ"):
        # å¼ºåˆ¶ç¦ç”¨ç¦»çº¿å‡ ä½•å¢å¼ºï¼ˆè®­ç»ƒæ—¶å·²åœ¨çº¿å¢å¼ºï¼‰
        result = process_image_set_with_overlap_filter(
            base_name, input_dir, train_folders, val_folders, test_folders,
            tile_size, overlap_ratio=overlap_ratio, size_tolerance=size_tolerance,
            apply_augmentation=False,  # ç¦ç”¨ç¦»çº¿å¢å¼º
            apply_h_flip=False, apply_v_flip=False,
            apply_rot90=False, apply_rot180=False, apply_rot270=False,
            overlap_threshold=overlap_threshold, split_assignment="train", global_boxes=train_global_boxes,
            filter_black_tiles=filter_black_tiles, black_threshold=black_threshold
        )
        if result:
            saved_count, generated_count = result
            if saved_count > 0:
                total_train_tiles += saved_count
                total_generated_train_tiles += generated_count
                processed_train_groups += 1

    # å¤„ç†éªŒè¯é›†
    total_val_tiles = 0
    total_generated_val_tiles = 0
    processed_val_groups = 0

    print("å¤„ç†éªŒè¯é›†...")
    for base_name in tqdm(val_base_names, desc="å¤„ç†éªŒè¯é›†å›¾åƒ"):
        result = process_image_set_with_overlap_filter(
            base_name, input_dir, train_folders, val_folders, test_folders,
            tile_size, overlap_ratio=overlap_ratio, size_tolerance=size_tolerance,
            apply_augmentation=False,  # éªŒè¯é›†åŒæ ·ç¦ç”¨ç¦»çº¿å¢å¼º
            apply_h_flip=False, apply_v_flip=False,
            apply_rot90=False, apply_rot180=False, apply_rot270=False,
            overlap_threshold=overlap_threshold, split_assignment="val", global_boxes=val_global_boxes,
            filter_black_tiles=filter_black_tiles, black_threshold=black_threshold
        )
        if result:
            saved_count, generated_count = result
            if saved_count > 0:
                total_val_tiles += saved_count
                total_generated_val_tiles += generated_count
                processed_val_groups += 1

    # ç»Ÿè®¡ç»“æœ
    total_tiles = total_train_tiles + total_val_tiles
    total_generated_tiles = total_generated_train_tiles + total_generated_val_tiles
    total_filtered_tiles = total_generated_tiles - total_tiles

    print(f"\nğŸ¯ åœ°ç†æ„ŸçŸ¥åˆ’åˆ†å®Œæˆ:" if geo_aware and SKLEARN_AVAILABLE else f"\nå¤„ç†å®Œæˆï¼")
    print(f"æˆåŠŸå¤„ç† {processed_train_groups + processed_val_groups}/{len(base_names)} ç»„å›¾åƒ")
    print(f"è®­ç»ƒé›†: {total_train_tiles} ä¸ªå°å— (æ¥è‡ª {len(train_base_names)} ä¸ªåŸå§‹å¤§å›¾)")
    print(f"éªŒè¯é›†: {total_val_tiles} ä¸ªå°å— (æ¥è‡ª {len(val_base_names)} ä¸ªåŸå§‹å¤§å›¾)")
    if create_test_folder:
        print(f"æµ‹è¯•é›†: {total_val_tiles} ä¸ªå°å— (ä¸éªŒè¯é›†ç›¸åŒ)")
    print(f"æ€»å…±ç”Ÿæˆ {total_generated_tiles} ä¸ªå°å—ï¼Œä¿å­˜ {total_tiles} ä¸ªå°å—")
    print(f"é‡å åº¦é˜ˆå€¼: {overlap_threshold * 100:.1f}%")
    print(f"æ€»å…±è¿‡æ»¤äº† {total_filtered_tiles} ä¸ªé‡å å°å—")

    if geo_aware and SKLEARN_AVAILABLE:
        print(f"\nğŸ” åœ°ç†åˆ†ç¦»éªŒè¯:")
        print(f"æœ€ç»ˆè®­ç»ƒé›†: {len(train_base_names)} ä¸ªåŸå§‹å¤§å›¾")
        print(f"æœ€ç»ˆéªŒè¯é›†: {len(val_base_names)} ä¸ªåŸå§‹å¤§å›¾")
        print(f"âœ… æ— é‡å ï¼Œåœ°ç†å®Œå…¨åˆ†ç¦»")


def verify_dataset_structure(dataset_path):
    """
    éªŒè¯æ•°æ®é›†ç»“æ„å®Œæ•´æ€§

    å‚æ•°:
        dataset_path: æ•°æ®é›†æ ¹ç›®å½•
    """
    # éªŒè¯æ–‡ä»¶å¤¹ç»“æ„
    required_folders = [
        os.path.join("train", "A"),
        os.path.join("train", "B"),
        os.path.join("train", "C"),
        os.path.join("train", "label"),
        os.path.join("val", "A"),
        os.path.join("val", "B"),
        os.path.join("val", "C"),
        os.path.join("val", "label")
    ]

    optional_folders = [
        os.path.join("test", "A"),
        os.path.join("test", "B"),
        os.path.join("test", "C"),
        os.path.join("test", "label")
    ]

    all_folders = required_folders + optional_folders

    folder_exists = {}
    for folder in all_folders:
        full_path = os.path.join(dataset_path, folder)
        folder_exists[folder] = os.path.exists(full_path)

    print("æ–‡ä»¶å¤¹ç»“æ„éªŒè¯:")
    for folder in required_folders:
        status = "âœ“" if folder_exists[folder] else "âœ—"
        print(f"  {status} {folder}")

    print("\nå¯é€‰æ–‡ä»¶å¤¹:")
    for folder in optional_folders:
        status = "âœ“" if folder_exists[folder] else "-"
        print(f"  {status} {folder}")

    # éªŒè¯æ–‡ä»¶æ•°é‡
    file_counts = {}
    for folder in all_folders:
        if folder_exists[folder]:
            full_path = os.path.join(dataset_path, folder)
            file_counts[folder] = len(os.listdir(full_path))

    print("\næ–‡ä»¶æ•°é‡éªŒè¯:")
    for folder in required_folders:
        if folder_exists[folder]:
            print(f"  {folder}: {file_counts[folder]} ä¸ªæ–‡ä»¶")

    # éªŒè¯æ–‡ä»¶åä¸€è‡´æ€§
    print("\næ–‡ä»¶åä¸€è‡´æ€§éªŒè¯:")

    def get_file_basenames(folder_path):
        if not os.path.exists(folder_path):
            return set()
        return {os.path.splitext(filename)[0] for filename in os.listdir(folder_path)}

    for split in ["train", "val", "test"]:
        if not all(folder_exists.get(os.path.join(split, subfolder), False) for subfolder in ["A", "B", "C", "label"]):
            print(f"  {split} æ–‡ä»¶å¤¹ä¸å®Œæ•´ï¼Œè·³è¿‡ä¸€è‡´æ€§æ£€æŸ¥")
            continue

        a_files = get_file_basenames(os.path.join(dataset_path, split, "A"))
        b_files = get_file_basenames(os.path.join(dataset_path, split, "B"))
        c_files = get_file_basenames(os.path.join(dataset_path, split, "C"))
        label_files = get_file_basenames(os.path.join(dataset_path, split, "label"))

        a_b_consistent = a_files == b_files
        a_c_consistent = a_files == c_files
        a_label_consistent = a_files == label_files

        print(f"  {split} é›†:")
        print(f"    A ä¸ B ä¸€è‡´: {'âœ“' if a_b_consistent else 'âœ—'}")
        print(f"    A ä¸ C ä¸€è‡´: {'âœ“' if a_c_consistent else 'âœ—'}")
        print(f"    A ä¸ label ä¸€è‡´: {'âœ“' if a_label_consistent else 'âœ—'}")

    print("\néªŒè¯å®Œæˆï¼")


def main():
    parser = argparse.ArgumentParser(description='å¤„ç†å˜åŒ–æ£€æµ‹æ•°æ®é›†å›¾åƒï¼Œåº”ç”¨é‡å è¿‡æ»¤ï¼Œå¹¶åˆ†å‰²ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†')
    parser.add_argument('--input_dir', type=str, default=DEFAULT_INPUT_DIR, help=f'è¾“å…¥ç›®å½• (é»˜è®¤: {DEFAULT_INPUT_DIR})')
    parser.add_argument('--output_dir', type=str, default=DEFAULT_OUTPUT_DIR, help=f'è¾“å‡ºç›®å½• (é»˜è®¤: {DEFAULT_OUTPUT_DIR})')
    parser.add_argument('--tile_size', type=int, default=DEFAULT_TILE_SIZE,
                        help=f'å°å—å¤§å° (é»˜è®¤: {DEFAULT_TILE_SIZE})')
    parser.add_argument('--overlap_ratio', type=float, default=DEFAULT_OVERLAP_RATIO,
                        help=f'é‡å æ¯”ä¾‹ (0-1ä¹‹é—´ï¼Œé»˜è®¤: {DEFAULT_OVERLAP_RATIO})')
    parser.add_argument('--overlap_threshold', type=float, default=DEFAULT_OVERLAP_THRESHOLD,
                        help=f'é‡å åº¦é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼çš„å°å—å°†è¢«ä¸¢å¼ƒ (0-1ä¹‹é—´ï¼Œé»˜è®¤: {DEFAULT_OVERLAP_THRESHOLD})')
    parser.add_argument('--val_ratio', type=float, default=DEFAULT_VAL_RATIO,
                        help=f'éªŒè¯é›†æ¯”ä¾‹ (0-1ä¹‹é—´ï¼Œé»˜è®¤: {DEFAULT_VAL_RATIO})')
    parser.add_argument('--size_tolerance', type=int, default=DEFAULT_SIZE_TOLERANCE,
                        help=f'å…è®¸çš„å›¾åƒå°ºå¯¸å·®å¼‚åƒç´ æ•° (é»˜è®¤: {DEFAULT_SIZE_TOLERANCE})')
    parser.add_argument('--no_augmentation', action='store_true',
                        help='ç¦ç”¨æ•°æ®å¢å¼º')
    parser.add_argument('--no_test', action='store_false', dest='create_test_folder',
                        help='ä¸åˆ›å»ºæµ‹è¯•é›†æ–‡ä»¶å¤¹')
    parser.add_argument('--apply_h_flip', action='store_true', default=APPLY_H_FLIP,
                        help=f'åº”ç”¨æ°´å¹³ç¿»è½¬ (é»˜è®¤: {APPLY_H_FLIP})')
    parser.add_argument('--apply_v_flip', action='store_true', default=APPLY_V_FLIP,
                        help=f'åº”ç”¨å‚ç›´ç¿»è½¬ (é»˜è®¤: {APPLY_V_FLIP})')
    parser.add_argument('--apply_rot90', action='store_true', default=APPLY_ROT90,
                        help=f'åº”ç”¨90Â°æ—‹è½¬ (é»˜è®¤: {APPLY_ROT90})')
    parser.add_argument('--apply_rot180', action='store_true', default=APPLY_ROT180,
                        help=f'åº”ç”¨180Â°æ—‹è½¬ (é»˜è®¤: {APPLY_ROT180})')
    parser.add_argument('--apply_rot270', action='store_true', default=APPLY_ROT270,
                        help=f'åº”ç”¨270Â°æ—‹è½¬ (é»˜è®¤: {APPLY_ROT270})')
    parser.add_argument('--no_filter_black', action='store_false', dest='filter_black_tiles',
                        help='ç¦ç”¨çº¯é»‘è‰²å°å—è¿‡æ»¤')
    parser.add_argument('--black_threshold', type=float, default=DEFAULT_BLACK_THRESHOLD,
                        help=f'çº¯é»‘è‰²åˆ¤å®šé˜ˆå€¼ (0-1ä¹‹é—´ï¼Œé»˜è®¤: {DEFAULT_BLACK_THRESHOLD})')
    parser.add_argument('--verify', action='store_true',
                        help='éªŒè¯è¾“å‡ºæ•°æ®é›†ç»“æ„')
    parser.add_argument('--geo_aware', action='store_true', default=DEFAULT_GEO_AWARE,
                        help=f'å¯ç”¨åœ°ç†æ„ŸçŸ¥åˆ’åˆ† (é»˜è®¤: {DEFAULT_GEO_AWARE})')
    parser.add_argument('--geo_eps', type=int, default=DEFAULT_GEO_EPS,
                        help=f'åœ°ç†èšç±»é‚»åŸŸåŠå¾„ (ç±³) (é»˜è®¤: {DEFAULT_GEO_EPS})')
    parser.add_argument('--geo_min_samples', type=int, default=DEFAULT_GEO_MIN_SAMPLES,
                        help=f'åœ°ç†èšç±»æœ€å°æ ·æœ¬æ•° (é»˜è®¤: {DEFAULT_GEO_MIN_SAMPLES})')


    args = parser.parse_args()

    # è®¾ç½®å‚æ•°
    tile_size = (args.tile_size, args.tile_size)
    overlap_ratio = args.overlap_ratio
    overlap_threshold = args.overlap_threshold
    val_ratio = args.val_ratio
    size_tolerance = args.size_tolerance
    # å¼ºåˆ¶ç¦ç”¨ç¦»çº¿æ•°æ®å¢å¼ºï¼šè®­ç»ƒé˜¶æ®µå·²æœ‰åœ¨çº¿å¢å¼ºï¼Œè¿™é‡Œä¿æŒåŸå›¾
    apply_augmentation = False
    create_test_folder = getattr(args, 'create_test_folder', DEFAULT_CREATE_TEST_FOLDER)
    apply_h_flip = False
    apply_v_flip = False
    apply_rot90 = False
    apply_rot180 = False
    apply_rot270 = False
    filter_black_tiles = getattr(args, 'filter_black_tiles', DEFAULT_FILTER_BLACK_TILES)
    black_threshold = args.black_threshold
    geo_aware = getattr(args, 'geo_aware', DEFAULT_GEO_AWARE)
    geo_eps = args.geo_eps
    geo_min_samples = args.geo_min_samples

    # å‚æ•°éªŒè¯
    if overlap_ratio < 0 or overlap_ratio >= 1:
        print(f"è­¦å‘Š: é‡å æ¯”ä¾‹å¿…é¡»åœ¨0-1ä¹‹é—´ï¼Œå½“å‰å€¼ {overlap_ratio} å°†è¢«é‡ç½®ä¸º {DEFAULT_OVERLAP_RATIO}")
        overlap_ratio = DEFAULT_OVERLAP_RATIO

    if overlap_threshold < 0 or overlap_threshold > 1:
        print(f"è­¦å‘Š: é‡å åº¦é˜ˆå€¼å¿…é¡»åœ¨0-1ä¹‹é—´ï¼Œå½“å‰å€¼ {overlap_threshold} å°†è¢«é‡ç½®ä¸º {DEFAULT_OVERLAP_THRESHOLD}")
        overlap_threshold = DEFAULT_OVERLAP_THRESHOLD

    if val_ratio < 0 or val_ratio > 1:
        print(f"è­¦å‘Š: éªŒè¯é›†æ¯”ä¾‹å¿…é¡»åœ¨0-1ä¹‹é—´ï¼Œå½“å‰å€¼ {val_ratio} å°†è¢«é‡ç½®ä¸º {DEFAULT_VAL_RATIO}")
        val_ratio = DEFAULT_VAL_RATIO

    if black_threshold < 0 or black_threshold > 1:
        print(f"è­¦å‘Š: çº¯é»‘è‰²åˆ¤å®šé˜ˆå€¼å¿…é¡»åœ¨0-1ä¹‹é—´ï¼Œå½“å‰å€¼ {black_threshold} å°†è¢«é‡ç½®ä¸º {DEFAULT_BLACK_THRESHOLD}")
        black_threshold = DEFAULT_BLACK_THRESHOLD

    # åœ°ç†æ„ŸçŸ¥å‚æ•°éªŒè¯
    if geo_eps <= 0:
        print(f"è­¦å‘Š: åœ°ç†èšç±»é‚»åŸŸåŠå¾„å¿…é¡»å¤§äº0ï¼Œå½“å‰å€¼ {geo_eps} å°†è¢«é‡ç½®ä¸º {DEFAULT_GEO_EPS}")
        geo_eps = DEFAULT_GEO_EPS
    
    if geo_min_samples < 1:
        print(f"è­¦å‘Š: åœ°ç†èšç±»æœ€å°æ ·æœ¬æ•°å¿…é¡»è‡³å°‘ä¸º1ï¼Œå½“å‰å€¼ {geo_min_samples} å°†è¢«é‡ç½®ä¸º {DEFAULT_GEO_MIN_SAMPLES}")
        geo_min_samples = DEFAULT_GEO_MIN_SAMPLES

    print(f"è¿è¡Œå‚æ•°:")
    print(f"  è¾“å…¥ç›®å½•: {args.input_dir}")
    print(f"  è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"  å°å—å¤§å°: {tile_size[0]}x{tile_size[1]}")
    print(f"  é‡å æ¯”ä¾‹: {overlap_ratio * 100:.1f}%")
    print(f"  é‡å åº¦é˜ˆå€¼: {overlap_threshold * 100:.1f}%")
    print(f"  éªŒè¯é›†æ¯”ä¾‹: {val_ratio * 100:.1f}%")
    print(f"  åˆ›å»ºæµ‹è¯•é›†: {'æ˜¯' if create_test_folder else 'å¦'}")
    print(f"  çº¯é»‘è‰²å°å—è¿‡æ»¤: {'å¯ç”¨' if filter_black_tiles else 'ç¦ç”¨'}")
    if filter_black_tiles:
        print(f"  çº¯é»‘è‰²åˆ¤å®šé˜ˆå€¼: {black_threshold * 100:.1f}%")
    print(f"  å…è®¸çš„å°ºå¯¸å·®å¼‚: {size_tolerance}åƒç´ ")
    print(f"  åˆ’åˆ†ç­–ç•¥: æµå¼tileçº§åˆ’åˆ†ï¼ˆæ•°é‡+å‰æ™¯æ¯”ä¾‹ï¼‰ï¼Œæ— åœ°ç†éš”ç¦»")
    print(f"  éšæœºç§å­: 666")
    print(f"  å‡ ä½•å˜æ¢æ•°æ®å¢å¼º: å·²ç¦ç”¨ (è®­ç»ƒæ—¶åœ¨çº¿å¢å¼º)")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)

    # å¤„ç†æ•°æ®é›†ï¼ˆæ–°æµç¨‹ï¼šæµå¼åˆ‡ç‰‡+å»é‡+åˆ’åˆ†ï¼Œä¸€æ¬¡è½ç›˜ï¼‰
    process_and_split_dataset_streaming(
        args.input_dir, args.output_dir, tile_size, overlap_ratio,
        size_tolerance, val_ratio, create_test_folder,
        overlap_threshold, filter_black_tiles, black_threshold,
        seed=666
    )

    # éªŒè¯è¾“å‡ºæ•°æ®é›†ç»“æ„
    if args.verify:
        print("\néªŒè¯è¾“å‡ºæ•°æ®é›†ç»“æ„:")
        verify_dataset_structure(args.output_dir)

    print("æ‰€æœ‰å¤„ç†å®Œæˆï¼")


if __name__ == "__main__":
    main() 