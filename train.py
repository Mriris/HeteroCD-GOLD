import time

import cv2
import torch.autograd
import torch.multiprocessing
from tensorboardX import SummaryWriter
from torch.utils.data import DataLoader

from models.HeteCD import TripleHeteCD
from options.train_options import TrainOptions
from utils.util import AverageMeter, get_confuse_matrix, cm2score
from utils.visualizer import Visualizer

torch.multiprocessing.set_sharing_strategy('file_system')

# Data and model choose
torch.set_num_threads(4)
import torch.nn.functional as FF
###############################################
from datasets import dataset
from datasets.dataset import *

def setup_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)

###############################################    
# Training options
###############################################

if __name__ == '__main__':
    opt = TrainOptions().parse()  # get training options
    
    # è®¾ç½®éšæœºæ•°ç§å­
    setup_seed(opt.seed)
    
    # ä½¿ç”¨optä¸­çš„é…ç½®åŠ è½½æ•°æ®é›†ï¼Œè®¾ç½®load_t2_opt=Trueæ¥åŠ è½½æ—¶é—´ç‚¹2çš„å…‰å­¦å›¾åƒ
    train_set_change = dataset.Data('train', root=opt.dataroot, opt=opt, load_t2_opt=True)
    train_loader_change = DataLoader(train_set_change, batch_size=opt.batch_size, num_workers=opt.num_workers, shuffle=True,
                                     drop_last=True)
    dataset_size = len(train_loader_change)
    
    # åŠ è½½éªŒè¯é›†ï¼ŒåŒæ ·åŠ è½½æ—¶é—´ç‚¹2çš„å…‰å­¦å›¾åƒ
    val_set = dataset.Data('val', root=opt.dataroot, load_t2_opt=True)
    val_loader = DataLoader(val_set, batch_size=opt.batch_size, num_workers=opt.num_workers, shuffle=False, drop_last=True)
    
    # åˆ›å»ºæ¨¡å‹ï¼Œè®¾ç½®use_distill=Trueå¯ç”¨è’¸é¦å­¦ä¹ 
    opt.use_distill = True  # å¯ç”¨è’¸é¦å­¦ä¹ 
    model = TripleHeteCD(opt, is_train=True)
    
    # æ·»åŠ æ–­ç‚¹ç»­è®­åŠŸèƒ½ï¼šæ¢å¤ä¹‹å‰çš„è®­ç»ƒçŠ¶æ€
    resume_epoch = 0
    best_iou = 0
    
    if opt.continue_train:
        # è¯»å–ä¹‹å‰çš„è®­ç»ƒè®°å½•
        training_info_path = os.path.join(opt.checkpoints_dir, opt.name, "training_info.txt")
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨è®­ç»ƒä¿¡æ¯æ–‡ä»¶
        if os.path.exists(training_info_path):
            with open(training_info_path, 'r') as f:
                lines = f.readlines()
                training_info = {}
                
                # è§£æè®­ç»ƒä¿¡æ¯æ–‡ä»¶ä¸­çš„æ‰€æœ‰é”®å€¼å¯¹
                for line in lines:
                    if ':' in line:
                        key, value = line.strip().split(':', 1)
                        training_info[key] = value
                
                try:
                    # è·å–ä¸‹ä¸€ä¸ªè¦è®­ç»ƒçš„epoch
                    if 'next_epoch' in training_info:
                        resume_epoch = int(training_info['next_epoch'])
                    elif 'current_epoch' in training_info:
                        # å¦‚æœæ²¡æœ‰next_epochï¼Œåˆ™ä»current_epoch + 1å¼€å§‹
                        resume_epoch = int(training_info['current_epoch']) + 1
                    
                    # è·å–å†å²æœ€ä½³IoU
                    if 'best_iou' in training_info:
                        best_iou = float(training_info['best_iou'])
                    
                    print(f"æ–­ç‚¹ç»­è®­ï¼šä»è½®æ¬¡ {resume_epoch} ç»§ç»­è®­ç»ƒï¼Œå†å²æœ€ä½³IoU: {best_iou:.4f}")
                    
                    # ç¡®å®šè¦åŠ è½½çš„æ¨¡å‹æ–‡ä»¶
                    # ä¼˜å…ˆä½¿ç”¨æŒ‡å®šçš„latest_model
                    if 'latest_model' in training_info:
                        opt.epoch = training_info['latest_model'].replace('_net_CD.pth', '')
                        print(f"å°†åŠ è½½æœ€æ–°æ¨¡å‹ï¼š{opt.epoch}")
                    else:
                        # å¦åˆ™ä½¿ç”¨current_epochä½œä¸ºåŠ è½½ç‚¹
                        if 'current_epoch' in training_info:
                            opt.epoch = training_info['current_epoch']
                            print(f"å°†åŠ è½½æŒ‡å®šepochæ¨¡å‹ï¼š{opt.epoch}")
                        else:
                            # æ²¡æœ‰æ˜ç¡®æŒ‡å®šï¼Œåˆ™å¯»æ‰¾æœ€æ–°çš„æ¨¡å‹
                            opt.epoch = 'latest'
                            print("å°†å°è¯•åŠ è½½æœ€æ–°æ¨¡å‹")
                except Exception as e:
                    print(f"è§£æè®­ç»ƒä¿¡æ¯æ–‡ä»¶å‡ºé”™: {e}")
                    print("è®­ç»ƒä¿¡æ¯æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")
                    resume_epoch = 0
                    best_iou = 0
                    opt.epoch = 'latest'
        else:
            print("æœªæ‰¾åˆ°è®­ç»ƒä¿¡æ¯æ–‡ä»¶ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")
    
    # è®¾ç½®æ¨¡å‹ï¼Œè¿™å°†åŠ è½½ä¿å­˜çš„æ¨¡å‹æƒé‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    model.setup(opt)
    visualizer = Visualizer(opt)
    
    # å¦‚æœæ˜¯æ–­ç‚¹ç»­è®­ï¼Œè¾“å‡ºç¡®è®¤ä¿¡æ¯
    if opt.continue_train and resume_epoch > 0:
        print(f"æ¨¡å‹å·²ä»{opt.epoch}åŠ è½½ï¼Œå°†ä»epoch {resume_epoch}ç»§ç»­è®­ç»ƒ")
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½
        param_sum = sum(p.sum().item() for p in model.netCD.parameters() if p.requires_grad)
        print(f"æ¨¡å‹å‚æ•°æ€»å’Œ: {param_sum:.4f} - {'æ­£å¸¸' if abs(param_sum) > 0.1 else 'è­¦å‘Š: å¯èƒ½æœªæ­£ç¡®åŠ è½½'}")
        
        # å¦‚æœæ¨¡å‹å‚æ•°å¼‚å¸¸ï¼Œç»™å‡ºæ›´è¯¦ç»†çš„è­¦å‘Š
        if abs(param_sum) <= 0.1:
            print("è­¦å‘Šï¼šæ¨¡å‹å‚æ•°æ€»å’Œæ¥è¿‘é›¶ï¼Œè¿™å¯èƒ½è¡¨æ˜æ¨¡å‹æœªæ­£ç¡®åŠ è½½ï¼")
            print("è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶è·¯å¾„å’Œæƒé‡æ˜¯å¦æ­£ç¡®ã€‚")
            response = input("æ¨¡å‹å‚æ•°å¯èƒ½æœ‰é—®é¢˜ï¼Œæ˜¯å¦ç»§ç»­è®­ç»ƒï¼Ÿ(y/n): ")
            if response.lower() != 'y':
                print("è®­ç»ƒå·²å–æ¶ˆ")
                exit(0)
    
    # åˆ›å»ºTensorBoardæ‘˜è¦å†™å…¥å™¨
    log_dir = os.path.join(opt.checkpoints_dir, opt.name, 'tensorboard_logs')
    os.makedirs(log_dir, exist_ok=True)
    writer = SummaryWriter(log_dir)
    
    # åœ¨TensorBoardä¸­è®°å½•è®­ç»ƒå‚æ•°
    opt_dict = vars(opt)
    for k in sorted(opt_dict.keys()):
        if isinstance(opt_dict[k], (int, float, str, bool)):
            writer.add_text('Parameters/' + k, str(opt_dict[k]), 0)

    # åˆ›å»ºæ—¥å¿—ç›®å½•
    if not os.path.exists(os.path.join(opt.checkpoints_dir, opt.name)):
        os.makedirs(os.path.join(opt.checkpoints_dir, opt.name))
    
    # æ—¥å¿—æ–‡ä»¶è·¯å¾„
    log_path = os.path.join(opt.checkpoints_dir, opt.name, "cd_log.txt")
    
    # å¦‚æœæ˜¯ç»§ç»­è®­ç»ƒä¸”æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæˆ–è€…æ˜¯æ–°è®­ç»ƒï¼Œåˆ™åˆ›å»ºæ–°çš„æ—¥å¿—æ–‡ä»¶
    if (not opt.continue_train) or (opt.continue_train and not os.path.exists(log_path)):
        with open(log_path, 'w') as f:
            f.write(f"HeteGAN è®­ç»ƒæ—¥å¿— - å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ•°æ®é›†: {opt.dataroot}\n")
            f.write(f"æ‰¹æ¬¡å¤§å°: {opt.batch_size}, å­¦ä¹ ç‡: {opt.lr}, GPU: {opt.gpu_ids}\n")
            f.write(f"æ˜¯å¦ä½¿ç”¨è’¸é¦å­¦ä¹ : {'æ˜¯' if opt.use_distill else 'å¦'}\n")
            
            # æ·»åŠ æ‰€æœ‰è®­ç»ƒå‚æ•°è®°å½•
            f.write("\n=== è®­ç»ƒå‚æ•° ===\n")
            # è·å–optä¸­çš„æ‰€æœ‰å±æ€§å¹¶æ’åº
            opt_dict = vars(opt)
            for k in sorted(opt_dict.keys()):
                f.write(f"{k}: {opt_dict[k]}\n")
            f.write("=== å‚æ•°ç»“æŸ ===\n\n")
            
            f.write("â”€" * 50 + "\n")

    total_iters = 0
    
    # å¦‚æœæ˜¯ç»§ç»­è®­ç»ƒï¼Œè®°å½•ç»­è®­ä¿¡æ¯åˆ°æ—¥å¿—æ–‡ä»¶
    if opt.continue_train and resume_epoch > 0:
        with open(log_path, 'a') as f:
            f.write('â”€' * 50 + f'\næ–­ç‚¹ç»­è®­ï¼šä»è½®æ¬¡ {resume_epoch} ç»§ç»­è®­ç»ƒï¼Œå†å²æœ€ä½³IoU: {best_iou:.4f}\n' + 'â”€' * 50 + '\n')

    # å°è¯•ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œä½†é»˜è®¤ä¸ºFalseé¿å…å‡ºç°NaNå€¼
    use_amp = opt.use_amp if hasattr(opt, 'use_amp') else False
    try:
        from torch.cuda.amp import GradScaler, autocast
        if use_amp:
            scaler = GradScaler()
            print("å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)")
        else:
            print("æ··åˆç²¾åº¦è®­ç»ƒå¯ç”¨ä½†æœªå¯ç”¨ã€‚å¦‚éœ€å¯ç”¨ï¼Œè¯·ä½¿ç”¨ --use_amp é€‰é¡¹")
    except ImportError:
        use_amp = False
        print("æ··åˆç²¾åº¦è®­ç»ƒä¸å¯ç”¨ - éœ€è¦PyTorch >= 1.6")
    
    # è·å–æ¢¯åº¦è£å‰ªå‚æ•°
    gradient_clip_norm = opt.gradient_clip_norm if hasattr(opt, 'gradient_clip_norm') else 1.0
    
    for epoch in range(resume_epoch,
                       opt.n_epochs):  # outer loop for different epochs; we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>
        epoch_start_time = time.time()  # timer for entire epoch
        iter_data_time = time.time()  # timer for data loading per iteration
        epoch_iter = 0  # the number of training iterations in current epoch, reset to 0 every epoch
        visualizer.reset()  # reset the visualizer: make sure it saves the results to HTML at least once every epoch
        
        # è®¾ç½®å½“å‰è½®æ¬¡åˆ°æ¨¡å‹ä¸­ï¼Œç”¨äºåŠ¨æ€æƒé‡è®¡ç®—
        if hasattr(model, 'set_epoch'):
            model.set_epoch(epoch)
        
        # æ‰“å°å½“å‰è½®æ¬¡å¼€å§‹ä¿¡æ¯
        print('\n' + '=' * 80)
        print(f'å¼€å§‹è®­ç»ƒç¬¬ {epoch}/{opt.n_epochs - 1} è½® | æ‰¹æ¬¡å¤§å°: {opt.batch_size} | å­¦ä¹ ç‡: {opt.lr}')
        print('=' * 80)
        
        # å­¦ç”Ÿç½‘ç»œçš„é¢„æµ‹ç»“æœå’Œæ ‡ç­¾
        preds_all = []
        labels_all = []
        names_all = []
        
        # æ•™å¸ˆç½‘ç»œçš„é¢„æµ‹ç»“æœå’Œæ ‡ç­¾
        teacher_preds_all = []
        teacher_labels_all = []
        
        for i, data in enumerate(train_loader_change):
            iter_start_time = time.time()  # timer for computation per iteration
            if total_iters % opt.print_freq == 0:
                t_data = iter_start_time - iter_data_time
            total_iters += opt.batch_size
            epoch_iter += opt.batch_size
            
            # è®¾ç½®æ¨¡å‹è¾“å…¥ï¼Œç°åœ¨åŒ…æ‹¬æ—¶é—´ç‚¹2çš„å…‰å­¦å›¾åƒ
            if len(data) == 5:  # å¸¦æ—¶é—´ç‚¹2çš„å…‰å­¦å›¾åƒçš„æƒ…å†µ
                model.set_input(data[0], data[1], data[2], data[3], opt.gpu_ids[0], data[4])
            else:  # ä¸å¸¦æ—¶é—´ç‚¹2çš„å…‰å­¦å›¾åƒçš„æƒ…å†µ
                model.set_input(data[0], data[1], data[2], data[3], opt.gpu_ids[0])

            # ä¼˜åŒ–æ¨¡å‹å‚æ•°
            model.optimizer_G.zero_grad()  # æ¸…ç©ºæ¢¯åº¦ç¼“å­˜
            
            if use_amp:
                # ä½¿ç”¨æ··åˆç²¾åº¦è¿›è¡Œå‰å‘å’Œåå‘ä¼ æ’­
                with autocast():
                    out_change = model.forward_CD()  # å‰å‘ä¼ æ’­
                    model.compute_losses()  # è®¡ç®—æŸå¤±ä½†ä¸ç«‹å³åå‘ä¼ æ’­
                
                # ä½¿ç”¨ç¼©æ”¾å™¨ç¼©æ”¾æŸå¤±å€¼ï¼Œé¿å…æ•°å€¼ä¸‹æº¢
                scaler.scale(model.loss_G).backward()
                
                # æ¢¯åº¦è£å‰ªé¿å…æ¢¯åº¦çˆ†ç‚¸
                scaler.unscale_(model.optimizer_G)
                torch.nn.utils.clip_grad_norm_(model.netCD.parameters(), max_norm=gradient_clip_norm)
                
                # æ›´æ–°æƒé‡
                scaler.step(model.optimizer_G)
                scaler.update()
            else:
                # å¸¸è§„è®­ç»ƒæµç¨‹
                out_change = model.optimize_parameters(epoch)
            
            # æ’å€¼åˆ°ç»Ÿä¸€å¤§å°
            out_change = FF.interpolate(out_change, size=(512, 512), mode='bilinear', align_corners=True)
            
            # è®°å½•å­¦ç”Ÿç½‘ç»œçš„é¢„æµ‹ç»“æœ
            with torch.no_grad():
                preds = torch.argmax(out_change, dim=1)
                pred_numpy = preds.cpu().numpy()
                labels_numpy = data[2].cpu().numpy()
                preds_all.append(pred_numpy)
                labels_all.append(labels_numpy)
                names_all.extend(data[3])
            
            # å¦‚æœä½¿ç”¨è’¸é¦å­¦ä¹ ä¸”æœ‰ç¬¬ä¸‰å¼ å›¾ï¼Œä¹Ÿè®°å½•æ•™å¸ˆç½‘ç»œçš„é¢„æµ‹ç»“æœ
            if opt.use_distill and len(data) == 5:
                with torch.no_grad():
                    teacher_pred, _ = model.get_teacher_pred()
                    if teacher_pred is not None:
                        teacher_pred = FF.interpolate(teacher_pred, size=(512, 512), mode='bilinear', align_corners=True)
                        teacher_preds = torch.argmax(teacher_pred, dim=1)
                        teacher_pred_numpy = teacher_preds.cpu().numpy()
                        teacher_preds_all.append(teacher_pred_numpy)
                        teacher_labels_all.append(labels_numpy)  # æ ‡ç­¾æ˜¯ç›¸åŒçš„

            if total_iters % opt.print_freq == 0:  # print training losses and save logging information to the disk
                losses = model.get_current_losses()
                t_comp = (time.time() - iter_start_time) / opt.batch_size
                # æ±‰åŒ–è¾“å‡ºæ ¼å¼
                loss_str = ' '.join([f'{name}: {value:.3f}' for name, value in losses.items()])
                visualizer.print_current_losses(epoch, epoch_iter, losses, t_comp, t_data)
                print(f'(è½®æ¬¡: {epoch}, æ‰¹æ¬¡: {i}, ç”¨æ—¶: {t_comp:.3f}ç§’/æ ·æœ¬, æ•°æ®åŠ è½½: {t_data:.3f}ç§’) {loss_str}')

            iter_data_time = time.time()
            
        # è¯„ä¼°å­¦ç”Ÿç½‘ç»œåœ¨è®­ç»ƒé›†ä¸Šçš„æ€§èƒ½
        preds_all = np.concatenate(preds_all, axis=0)
        labels_all = np.concatenate(labels_all, axis=0)
        hist = get_confuse_matrix(2, labels_all, preds_all)
        score = cm2score(hist)
        
        # è®°å½•è®­ç»ƒæŒ‡æ ‡åˆ°TensorBoard
        writer.add_scalar('Train/Student/Accuracy', score['acc'], epoch)
        writer.add_scalar('Train/Student/MeanIoU', score['miou'], epoch)
        writer.add_scalar('Train/Student/IoU_0', score['iou_0'], epoch)
        writer.add_scalar('Train/Student/IoU_1', score['iou_1'], epoch)
        writer.add_scalar('Train/Student/F1_0', score['F1_0'], epoch)
        writer.add_scalar('Train/Student/F1_1', score['F1_1'], epoch)
        
        # è®°å½•è®­ç»ƒæŸå¤±
        for loss_name, loss_value in model.get_current_losses().items():
            writer.add_scalar(f'Train/Loss_{loss_name}', loss_value, epoch)
            
        print('å­¦ç”Ÿç½‘ç»œè®­ç»ƒè¯„åˆ†: %s' % {key: score[key] for key in score})
        
        # è®°å½•è®­ç»ƒç»“æœ
        train_score = score
        train_iou = score['iou_1']  # ä¿å­˜è®­ç»ƒé›†ä¸Šçš„iou_1
        
        # è®°å½•æ•™å¸ˆç½‘ç»œåœ¨è®­ç»ƒé›†ä¸Šçš„æ€§èƒ½ï¼ˆå¦‚æœæœ‰ï¼‰
        teacher_score = None
        if opt.use_distill and len(teacher_preds_all) > 0:
            teacher_preds_all = np.concatenate(teacher_preds_all, axis=0)
            teacher_labels_all = np.concatenate(teacher_labels_all, axis=0)
            hist = get_confuse_matrix(2, teacher_labels_all, teacher_preds_all)
            teacher_score = cm2score(hist)
            
            # è®°å½•æ•™å¸ˆç½‘ç»œè®­ç»ƒæŒ‡æ ‡åˆ°TensorBoard
            writer.add_scalar('Train/Teacher/Accuracy', teacher_score['acc'], epoch)
            writer.add_scalar('Train/Teacher/MeanIoU', teacher_score['miou'], epoch)
            writer.add_scalar('Train/Teacher/IoU_0', teacher_score['iou_0'], epoch)
            writer.add_scalar('Train/Teacher/IoU_1', teacher_score['iou_1'], epoch)
            writer.add_scalar('Train/Teacher/F1_0', teacher_score['F1_0'], epoch)
            writer.add_scalar('Train/Teacher/F1_1', teacher_score['F1_1'], epoch)
            
            print('æ•™å¸ˆç½‘ç»œè®­ç»ƒè¯„åˆ†: %s' % {key: teacher_score[key] for key in teacher_score})

        # è·å–è®­ç»ƒæŸå¤±
        train_losses = model.get_current_losses()
        train_loss = sum(train_losses.values()) if train_losses else 0

        # åˆ›å»ºä¿å­˜é¢„æµ‹ç»“æœçš„ç›®å½•
        best_preds_dir = os.path.join(opt.checkpoints_dir, opt.name, "results")
        if not os.path.exists(best_preds_dir):
            os.makedirs(best_preds_dir)
            
        # éªŒè¯é›†è¯„ä¼°
        val_loss = AverageMeter()
        preds_all_val = []
        labels_all_val = []
        names_all_val = []
        
        # æ•™å¸ˆç½‘ç»œçš„éªŒè¯é›†é¢„æµ‹
        teacher_preds_all_val = []
        teacher_labels_all_val = []
        teacher_val_loss = AverageMeter()
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        model.netCD.eval()
        
        for i, data in enumerate(val_loader):
            # è®¾ç½®æ¨¡å‹è¾“å…¥ï¼Œç°åœ¨åŒ…æ‹¬æ—¶é—´ç‚¹2çš„å…‰å­¦å›¾åƒ
            if len(data) == 5:  # å¸¦æ—¶é—´ç‚¹2çš„å…‰å­¦å›¾åƒçš„æƒ…å†µ
                model.set_input(data[0], data[1], data[2], data[3], opt.gpu_ids[0], data[4])
            else:  # ä¸å¸¦æ—¶é—´ç‚¹2çš„å…‰å­¦å›¾åƒçš„æƒ…å†µ
                model.set_input(data[0], data[1], data[2], data[3], opt.gpu_ids[0])
                
            # è·å–å­¦ç”Ÿç½‘ç»œé¢„æµ‹ç»“æœ
            with torch.no_grad():
                out_change, loss = model.get_val_pred()
                out_change = FF.interpolate(out_change, size=(512, 512), mode='bilinear', align_corners=True)
                val_loss.update(loss.cpu().detach().numpy())
                preds = torch.argmax(out_change, dim=1)
                pred_numpy = preds.cpu().numpy()
                labels_numpy = data[2].cpu().numpy()
                preds_all_val.append(pred_numpy)
                labels_all_val.append(labels_numpy)
                names_all_val.extend(data[3])
                
                # å¦‚æœä½¿ç”¨è’¸é¦å­¦ä¹ ä¸”æœ‰ç¬¬ä¸‰å¼ å›¾åƒï¼Œè·å–æ•™å¸ˆç½‘ç»œé¢„æµ‹ç»“æœ
                if opt.use_distill and len(data) == 5:
                    teacher_pred, teacher_loss = model.get_teacher_pred()
                    if teacher_pred is not None:
                        teacher_pred = FF.interpolate(teacher_pred, size=(512, 512), mode='bilinear', align_corners=True)
                        teacher_val_loss.update(teacher_loss.cpu().detach().numpy())
                        teacher_preds = torch.argmax(teacher_pred, dim=1)
                        teacher_pred_numpy = teacher_preds.cpu().numpy()
                        teacher_preds_all_val.append(teacher_pred_numpy)
                        teacher_labels_all_val.append(labels_numpy)  # æ ‡ç­¾æ˜¯ç›¸åŒçš„
        
        # è¯„ä¼°å­¦ç”Ÿç½‘ç»œåœ¨éªŒè¯é›†ä¸Šçš„æ€§èƒ½
        preds_all_val = np.concatenate(preds_all_val, axis=0)
        labels_all_val = np.concatenate(labels_all_val, axis=0)
        hist = get_confuse_matrix(2, labels_all_val, preds_all_val)
        score = cm2score(hist)
        
        # è®°å½•éªŒè¯æŒ‡æ ‡åˆ°TensorBoard
        writer.add_scalar('Validation/Student/Accuracy', score['acc'], epoch)
        writer.add_scalar('Validation/Student/MeanIoU', score['miou'], epoch)
        writer.add_scalar('Validation/Student/IoU_0', score['iou_0'], epoch)
        writer.add_scalar('Validation/Student/IoU_1', score['iou_1'], epoch)
        writer.add_scalar('Validation/Student/F1_0', score['F1_0'], epoch)
        writer.add_scalar('Validation/Student/F1_1', score['F1_1'], epoch)
        writer.add_scalar('Validation/Student/Loss', val_loss.average(), epoch)
        
        # è®°å½•éªŒè¯é›†ä¸Šçš„IoUï¼Œç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½
        val_iou = score['iou_1']
        
        # è¯„ä¼°æ•™å¸ˆç½‘ç»œåœ¨éªŒè¯é›†ä¸Šçš„æ€§èƒ½ï¼ˆå¦‚æœæœ‰ï¼‰
        teacher_val_score = None
        if opt.use_distill and len(teacher_preds_all_val) > 0:
            teacher_preds_all_val = np.concatenate(teacher_preds_all_val, axis=0)
            teacher_labels_all_val = np.concatenate(teacher_labels_all_val, axis=0)
            hist = get_confuse_matrix(2, teacher_labels_all_val, teacher_preds_all_val)
            teacher_val_score = cm2score(hist)
            
            # è®°å½•æ•™å¸ˆç½‘ç»œéªŒè¯æŒ‡æ ‡åˆ°TensorBoard
            writer.add_scalar('Validation/Teacher/Accuracy', teacher_val_score['acc'], epoch)
            writer.add_scalar('Validation/Teacher/MeanIoU', teacher_val_score['miou'], epoch)
            writer.add_scalar('Validation/Teacher/IoU_0', teacher_val_score['iou_0'], epoch)
            writer.add_scalar('Validation/Teacher/IoU_1', teacher_val_score['iou_1'], epoch)
            writer.add_scalar('Validation/Teacher/F1_0', teacher_val_score['F1_0'], epoch)
            writer.add_scalar('Validation/Teacher/F1_1', teacher_val_score['F1_1'], epoch)
            writer.add_scalar('Validation/Teacher/Loss', teacher_val_loss.average(), epoch)
            
            print('æ•™å¸ˆç½‘ç»œéªŒè¯è¯„åˆ†: %s' % {key: teacher_val_score[key] for key in teacher_val_score})
        
        # ä¿å­˜æ¨¡å‹å‰æ¸…ç†æ—§çš„éæœ€ä½³æ¨¡å‹æ–‡ä»¶ï¼Œåªä¿ç•™æœ€æ–°å’Œæœ€å¥½çš„
        model_dir = os.path.join(opt.checkpoints_dir, opt.name)
        # æ‰¾å‡ºå½“å‰ç›®å½•ä¸‹æ‰€æœ‰æ¨¡å‹æ–‡ä»¶
        model_files = [f for f in os.listdir(model_dir) 
                      if f.endswith('_net_CD.pth') and not f.startswith('best')]
        # æŒ‰ç…§epochç¼–å·æ’åº
        if len(model_files) > 1:  # å¦‚æœæœ‰å¤šä¸ªéæœ€ä½³æ¨¡å‹æ–‡ä»¶
            # æ’åºæ¨¡å‹æ–‡ä»¶ï¼Œä¿ç•™æœ€æ–°çš„ï¼Œåˆ é™¤å…¶ä»–çš„
            model_files.sort(key=lambda x: int(x.split('_')[0]) if x.split('_')[0].isdigit() else -1, reverse=True)
            latest_model = model_files[0]  # ä¿ç•™æœ€æ–°çš„æ¨¡å‹
            for model_file in model_files[1:]:  # åˆ é™¤å…¶ä»–æ—§æ¨¡å‹
                try:
                    os.remove(os.path.join(model_dir, model_file))
                    print(f"å·²åˆ é™¤æ—§æ¨¡å‹æ–‡ä»¶: {model_file}")
                except Exception as e:
                    print(f"åˆ é™¤æ–‡ä»¶ {model_file} æ—¶å‡ºé”™: {e}")

        # ä¿å­˜å½“å‰æ¨¡å‹ä½œä¸ºæœ€æ–°æ¨¡å‹
        model.save_networks(epoch)
        print(f"å·²ä¿å­˜æœ€æ–°æ¨¡å‹: {epoch}_net_CD.pth")
        latest_model_file = f"{epoch}_net_CD.pth"
        
        # æ˜¯å¦éœ€è¦ä¿å­˜æœ€ä½³æ¨¡å‹
        best_model_updated = False
        if val_iou > best_iou:
            with open(os.path.join(opt.checkpoints_dir, opt.name, "cd_log.txt"), 'a') as f:
                f.write(f'æ–°çºªå½•ï¼ä¿å­˜æ¨¡å‹è‡³: {os.path.join(opt.checkpoints_dir, opt.name)}\n')
            
            # æŸ¥æ‰¾å¹¶åˆ é™¤ä¹‹å‰çš„æœ€ä½³æ¨¡å‹æ–‡ä»¶
            for file in os.listdir(model_dir):
                if file.endswith('.pth') and file.startswith('best_net'):
                    try:
                        os.remove(os.path.join(model_dir, file))
                        print(f"å·²åˆ é™¤æ—§çš„æœ€ä½³æ¨¡å‹: {file}")
                    except Exception as e:
                        print(f"åˆ é™¤æ–‡ä»¶ {file} æ—¶å‡ºé”™: {e}")
            
            # ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹
            model.save_networks('best')
            best_iou = val_iou
            best_epoch = epoch  # è®°å½•äº§ç”Ÿæœ€ä½³ç»“æœçš„epoch
            best_model_file = f"best_net_CD.pth"
            best_model_updated = True
            
            # ä¿å­˜æœ€ä½³ç»“æœé¢„æµ‹å›¾
            for i in range(len(names_all_val)):
                save_path = os.path.join(best_preds_dir, names_all_val[i])
                cv2.imwrite(save_path, preds_all_val[i] * 255)
            
            print('ğŸŒŸ æ›´æ–°æœ€ä½³IoUæ¨¡å‹ ğŸŒŸ')
        
        # æ›´æ–°è®­ç»ƒä¿¡æ¯æ–‡ä»¶ï¼Œç¡®ä¿ä¿¡æ¯å®Œæ•´
        with open(os.path.join(opt.checkpoints_dir, opt.name, "training_info.txt"), 'w') as f:
            # ä¸‹ä¸€ä¸ªè¦è®­ç»ƒçš„epoch
            next_epoch = epoch + 1
            f.write(f"next_epoch:{next_epoch}\n")
            # å½“å‰å·²å®Œæˆçš„epoch
            f.write(f"current_epoch:{epoch}\n")
            # æœ€ä½³æ€§èƒ½åŠå¯¹åº”epoch
            f.write(f"best_iou:{best_iou}\n")
            if 'best_epoch' in locals():
                f.write(f"best_epoch:{best_epoch}\n")
            # æœ€æ–°æ¨¡å‹æ–‡ä»¶å
            f.write(f"latest_model:{latest_model_file}\n")
            # æœ€ä½³æ¨¡å‹æ–‡ä»¶å
            f.write(f"best_model:best_net_CD.pth\n")
            # è®°å½•æœ€åä¸€æ¬¡æ›´æ–°æœ€ä½³æ¨¡å‹çš„è½®æ¬¡
            if best_model_updated:
                f.write(f"best_model_updated_at:{epoch}\n")
            
        with open(os.path.join(opt.checkpoints_dir, opt.name, "cd_log.txt"), 'a') as f:
            # æ·»åŠ åˆ†éš”è¡Œ
            f.write('='*100 + '\nã€Epoch: %dã€‘\n' % epoch)
            
            # å¦‚æœä½¿ç”¨åŠ¨æ€æƒé‡ï¼Œè®°å½•å½“å‰æƒé‡å€¼
            if opt.use_dynamic_weights and hasattr(model, 'get_dynamic_weights'):
                model.set_epoch(epoch)  # ç¡®ä¿æ¨¡å‹çŸ¥é“å½“å‰epoch
                cd_weight, distill_weight, diff_att_weight = model.get_dynamic_weights()
                f.write(f'ã€åŠ¨æ€æƒé‡ã€‘CDæŸå¤±: {cd_weight:.4f}, è’¸é¦æŸå¤±: {distill_weight:.4f}, å·®å¼‚å›¾æ³¨æ„åŠ›æŸå¤±: {diff_att_weight:.4f}\n')
            
            # åˆå¹¶å±•ç¤ºè®­ç»ƒå’ŒéªŒè¯ç»“æœ
            f.write('ã€å­¦ç”Ÿç½‘ç»œã€‘ - è®­ç»ƒIoU: %.4f (Loss: %.4f) | éªŒè¯IoU: %.4f/%.4f (Loss: %.4f)\n' %
                   (train_iou, train_loss, score['iou_1'], best_iou, val_loss.average()))
                   
            # å¦‚æœæœ‰æ•™å¸ˆç½‘ç»œï¼Œè®°å½•æ•™å¸ˆç½‘ç»œç»“æœ
            if teacher_score is not None and teacher_val_score is not None:
                f.write('ã€æ•™å¸ˆç½‘ç»œã€‘ - è®­ç»ƒIoU: %.4f | éªŒè¯IoU: %.4f (Loss: %.4f)\n' %
                       (teacher_score['iou_1'], teacher_val_score['iou_1'], teacher_val_loss.average()))

            # åˆ†åˆ«è®°å½•è¯¦ç»†æŒ‡æ ‡
            f.write('å­¦ç”Ÿç½‘ç»œè®­ç»ƒè¯¦ç»†æŒ‡æ ‡: %s\n' % {k: round(v, 4) if isinstance(v, float) else v for k, v in train_score.items()})
            f.write('å­¦ç”Ÿç½‘ç»œéªŒè¯è¯¦ç»†æŒ‡æ ‡: %s\n' % {k: round(v, 4) if isinstance(v, float) else v for k, v in score.items()})
            
            # å¦‚æœæœ‰æ•™å¸ˆç½‘ç»œï¼Œè®°å½•æ•™å¸ˆç½‘ç»œè¯¦ç»†æŒ‡æ ‡
            if teacher_score is not None and teacher_val_score is not None:
                f.write('æ•™å¸ˆç½‘ç»œè®­ç»ƒè¯¦ç»†æŒ‡æ ‡: %s\n' % {k: round(v, 4) if isinstance(v, float) else v for k, v in teacher_score.items()})
                f.write('æ•™å¸ˆç½‘ç»œéªŒè¯è¯¦ç»†æŒ‡æ ‡: %s\n' % {k: round(v, 4) if isinstance(v, float) else v for k, v in teacher_val_score.items()})

        # ç¾åŒ–æ§åˆ¶å°è¾“å‡º
        print('='*100)
        # å¦‚æœä½¿ç”¨åŠ¨æ€æƒé‡ï¼Œæ‰“å°å½“å‰æƒé‡å€¼
        if opt.use_dynamic_weights and hasattr(model, 'get_dynamic_weights'):
            model.set_epoch(epoch)  # ç¡®ä¿æ¨¡å‹çŸ¥é“å½“å‰epoch
            cd_weight, distill_weight, diff_att_weight = model.get_dynamic_weights()
            print(f'ã€åŠ¨æ€æƒé‡ã€‘CDæŸå¤±: {cd_weight:.4f}, è’¸é¦æŸå¤±: {distill_weight:.4f}, å·®å¼‚å›¾æ³¨æ„åŠ›æŸå¤±: {diff_att_weight:.4f}')

        # åˆå¹¶å±•ç¤ºè®­ç»ƒå’ŒéªŒè¯ç»“æœ
        print('ã€Epoch: %dã€‘å­¦ç”Ÿç½‘ç»œ - è®­ç»ƒIoU: %.4f (Loss: %.4f) | éªŒè¯IoU: %.4f/%.4f (Loss: %.4f)' %
             (epoch, train_iou, train_loss, score['iou_1'], best_iou, val_loss.average()))
             
        # å¦‚æœæœ‰æ•™å¸ˆç½‘ç»œï¼Œæ‰“å°æ•™å¸ˆç½‘ç»œç»“æœ
        if teacher_score is not None and teacher_val_score is not None:
            print('ã€Epoch: %dã€‘æ•™å¸ˆç½‘ç»œ - è®­ç»ƒIoU: %.4f | éªŒè¯IoU: %.4f (Loss: %.4f)' %
                 (epoch, teacher_score['iou_1'], teacher_val_score['iou_1'], teacher_val_loss.average()))

        # å¯¹æ¯”å±•ç¤ºå…³é”®æŒ‡æ ‡ - ä½¿ç”¨å›ºå®šå®½åº¦ç¡®ä¿å¯¹é½
        print('â•”â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—')
        print('â•‘ ç½‘ç»œç±»å‹ â•‘    èŠ‚ç‚¹    â•‘     å‡†ç¡®ç‡     â•‘    å¹³å‡IoU     â•‘    å¹³å‡F1      â•‘')
        print('â• â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£')
        print('â•‘  å­¦ç”Ÿç½‘ç»œ â•‘   è®­ç»ƒé›†   â•‘     %-7.4f   â•‘     %-7.4f   â•‘     %-7.4f   â•‘' %
             (train_score['acc'], train_score['miou'], train_score['mf1']))
        print('â•‘  å­¦ç”Ÿç½‘ç»œ â•‘   éªŒè¯é›†   â•‘     %-7.4f   â•‘     %-7.4f   â•‘     %-7.4f   â•‘' %
             (score['acc'], score['miou'], score['mf1']))
             
        # å¦‚æœæœ‰æ•™å¸ˆç½‘ç»œï¼Œæ‰“å°æ•™å¸ˆç½‘ç»œå¯¹æ¯”
        if teacher_score is not None and teacher_val_score is not None:
            print('â•‘  æ•™å¸ˆç½‘ç»œ â•‘   è®­ç»ƒé›†   â•‘     %-7.4f   â•‘     %-7.4f   â•‘     %-7.4f   â•‘' %
                 (teacher_score['acc'], teacher_score['miou'], teacher_score['mf1']))
            print('â•‘  æ•™å¸ˆç½‘ç»œ â•‘   éªŒè¯é›†   â•‘     %-7.4f   â•‘     %-7.4f   â•‘     %-7.4f   â•‘' %
                 (teacher_val_score['acc'], teacher_val_score['miou'], teacher_val_score['mf1']))
        print('â•šâ•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•')

        # å¦‚æœéªŒè¯é›†IoUä¼˜äºä¹‹å‰æœ€ä½³å€¼ï¼Œæ˜¾ç¤ºæç¤º
        if score['iou_1'] >= best_iou - 0.0001:  # è€ƒè™‘æµ®ç‚¹ç²¾åº¦
            print('ğŸŒŸ æœ¬è½®éªŒè¯IoUåˆ›å»ºæ–°é«˜ï¼')

        print('è®­ç»ƒè½®æ¬¡ %d / %d ç»“æŸ \t è€—æ—¶: %d ç§’' % (epoch, opt.n_epochs, time.time() - epoch_start_time))

        # åœ¨æ¯ä¸ªepochç»“æŸæ—¶æ›´æ–°å­¦ä¹ ç‡
        model.update_learning_rate()
        
    # å…³é—­TensorBoardå†™å…¥å™¨
    writer.close()
