import os
import time
import random
import numpy as np
import torch.nn as nn
import torch.autograd
from skimage import io
from torch import optim
import cv2

from tensorboardX import SummaryWriter
from torch.utils.data import DataLoader
from options.train_options import TrainOptions
from models import create_model
from models.HeteGAN import Pix2PixModel
from utils.visualizer import Visualizer
from utils.util import accuracy, SCDD_eval_all, AverageMeter, get_confuse_matrix, cm2score

import torch.multiprocessing

torch.multiprocessing.set_sharing_strategy('file_system')

# Data and model choose
torch.set_num_threads(4)
import torch.nn.functional as FF
###############################################
from datasets import dataset
from datasets.dataset import *

def setup_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)

###############################################    
# Training options
###############################################

if __name__ == '__main__':
    opt = TrainOptions().parse()  # get training options
    
    # è®¾ç½®éšæœºæ•°ç§å­
    setup_seed(opt.seed)
    
    # ä½¿ç”¨optä¸­çš„é…ç½®åŠ è½½æ•°æ®é›†
    train_set_change = dataset.Data('train', root=opt.dataroot, opt=opt)
    train_loader_change = DataLoader(train_set_change, batch_size=opt.batch_size, num_workers=opt.num_workers, shuffle=True,
                                     drop_last=True)
    dataset_size = len(train_loader_change)
    val_set = dataset.Data('val', root=opt.dataroot)
    val_loader = DataLoader(val_set, batch_size=opt.batch_size, num_workers=opt.num_workers, shuffle=False, drop_last=True)
    model = Pix2PixModel(opt, is_train=True)
    model.setup(opt)
    visualizer = Visualizer(opt)
    
    # åˆ›å»ºTensorBoardæ‘˜è¦å†™å…¥å™¨
    log_dir = os.path.join(opt.checkpoints_dir, opt.name, 'tensorboard_logs')
    os.makedirs(log_dir, exist_ok=True)
    writer = SummaryWriter(log_dir)
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    if not os.path.exists(os.path.join(opt.checkpoints_dir, opt.name)):
        os.makedirs(os.path.join(opt.checkpoints_dir, opt.name))
    
    # æ—¥å¿—æ–‡ä»¶è·¯å¾„
    log_path = os.path.join(opt.checkpoints_dir, opt.name, "cd_log.txt")
    
    # å¦‚æœæ˜¯ç»§ç»­è®­ç»ƒä¸”æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæˆ–è€…æ˜¯æ–°è®­ç»ƒï¼Œåˆ™åˆ›å»ºæ–°çš„æ—¥å¿—æ–‡ä»¶
    if (not opt.continue_train) or (opt.continue_train and not os.path.exists(log_path)):
        with open(log_path, 'w') as f:
            f.write(f"HeteGAN è®­ç»ƒæ—¥å¿— - å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ•°æ®é›†: {opt.dataroot}\n")
            f.write(f"æ‰¹æ¬¡å¤§å°: {opt.batch_size}, å­¦ä¹ ç‡: {opt.lr}, GPU: {opt.gpu_ids}\n")
            f.write("â”€" * 50 + "\n")

    total_iters = 0
    resume_epoch = 0
    best_iou = 0
    
    # æ·»åŠ æ–­ç‚¹ç»­è®­åŠŸèƒ½ï¼šæ¢å¤ä¹‹å‰çš„è®­ç»ƒçŠ¶æ€
    if opt.continue_train:
        # è¯»å–ä¹‹å‰çš„è®­ç»ƒè®°å½•
        training_info_path = os.path.join(opt.checkpoints_dir, opt.name, "training_info.txt")
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨è®­ç»ƒä¿¡æ¯æ–‡ä»¶
        if os.path.exists(training_info_path):
            with open(training_info_path, 'r') as f:
                lines = f.readlines()
                if len(lines) >= 2:
                    try:
                        resume_epoch = int(lines[0].strip().split(':')[1])
                        best_iou = float(lines[1].strip().split(':')[1])
                        print(f"æ–­ç‚¹ç»­è®­ï¼šä»è½®æ¬¡ {resume_epoch} ç»§ç»­è®­ç»ƒï¼Œå†å²æœ€ä½³IoU: {best_iou:.4f}")
                        
                        # è®°å½•ç»­è®­ä¿¡æ¯åˆ°æ—¥å¿—æ–‡ä»¶
                        with open(os.path.join(opt.checkpoints_dir, opt.name, "cd_log.txt"), 'a') as f:
                            f.write('â”€' * 50 + f'\næ–­ç‚¹ç»­è®­ï¼šä»è½®æ¬¡ {resume_epoch} ç»§ç»­è®­ç»ƒï¼Œå†å²æœ€ä½³IoU: {best_iou:.4f}\n' + 'â”€' * 50 + '\n')
                    except:
                        print("è®­ç»ƒä¿¡æ¯æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")
                        resume_epoch = 0
                        best_iou = 0
        else:
            print("æœªæ‰¾åˆ°è®­ç»ƒä¿¡æ¯æ–‡ä»¶ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")

    for epoch in range(resume_epoch,
                       opt.n_epochs):  # outer loop for different epochs; we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>
        epoch_start_time = time.time()  # timer for entire epoch
        iter_data_time = time.time()  # timer for data loading per iteration
        epoch_iter = 0  # the number of training iterations in current epoch, reset to 0 every epoch
        visualizer.reset()  # reset the visualizer: make sure it saves the results to HTML at least once every epoch
        
        # æ‰“å°å½“å‰è½®æ¬¡å¼€å§‹ä¿¡æ¯
        print('\n' + '=' * 80)
        print(f'å¼€å§‹è®­ç»ƒç¬¬ {epoch}/{opt.n_epochs - 1} è½® | æ‰¹æ¬¡å¤§å°: {opt.batch_size} | å­¦ä¹ ç‡: {opt.lr}')
        print('=' * 80)
        
        preds_all = []
        labels_all = []
        names_all = []
        for i, data in enumerate(train_loader_change):
            iter_start_time = time.time()  # timer for computation per iteration
            if total_iters % opt.print_freq == 0:
                t_data = iter_start_time - iter_data_time
            total_iters += opt.batch_size
            epoch_iter += opt.batch_size
            model.set_input(data[0], data[1], data[2], data[3],
                            opt.gpu_ids[0])  # unpack data from dataset and apply preprocessing

            out_change = model.optimize_parameters(
                epoch)  # calculate loss functions, get gradients, update network weights
            out_change = FF.interpolate(out_change, size=(512, 512), mode='bilinear', align_corners=True)
            preds = torch.argmax(out_change, dim=1)
            pred_numpy = preds.cpu().numpy()
            labels_numpy = data[2].cpu().numpy()
            preds_all.append(pred_numpy)
            labels_all.append(labels_numpy)
            names_all.extend(data[3])

            if total_iters % opt.print_freq == 0:  # print training losses and save logging information to the disk
                losses = model.get_current_losses()
                t_comp = (time.time() - iter_start_time) / opt.batch_size
                # æ±‰åŒ–è¾“å‡ºæ ¼å¼
                loss_str = ' '.join([f'{name}: {value:.3f}' for name, value in losses.items()])
                visualizer.print_current_losses(epoch, epoch_iter, losses, t_comp, t_data)
                print(f'(è½®æ¬¡: {epoch}, æ‰¹æ¬¡: {i}, ç”¨æ—¶: {t_comp:.3f}ç§’/æ ·æœ¬, æ•°æ®åŠ è½½: {t_data:.3f}ç§’) {loss_str}')

            iter_data_time = time.time()
        preds_all = np.concatenate(preds_all, axis=0)
        labels_all = np.concatenate(labels_all, axis=0)
        hist = get_confuse_matrix(2, labels_all, preds_all)
        score = cm2score(hist)
        
        # è®°å½•è®­ç»ƒæŒ‡æ ‡åˆ°TensorBoard
        writer.add_scalar('Train/Accuracy', score['acc'], epoch)
        writer.add_scalar('Train/MeanIoU', score['miou'], epoch)
        writer.add_scalar('Train/IoU_0', score['iou_0'], epoch)
        writer.add_scalar('Train/IoU_1', score['iou_1'], epoch)
        writer.add_scalar('Train/F1_0', score['F1_0'], epoch)
        writer.add_scalar('Train/F1_1', score['F1_1'], epoch)
        
        # è®°å½•è®­ç»ƒæŸå¤±
        for loss_name, loss_value in model.get_current_losses().items():
            writer.add_scalar(f'Train/Loss_{loss_name}', loss_value, epoch)
            
        print('è®­ç»ƒè½®æ¬¡: %d è¯„åˆ†: %s' % (epoch, {key: score[key] for key in score}))

        # è®°å½•è®­ç»ƒç»“æœ
        train_score = score
        train_iou = score['iou_1']  # ä¿å­˜è®­ç»ƒé›†ä¸Šçš„iou_1

        # è·å–è®­ç»ƒæŸå¤±
        train_losses = model.get_current_losses()
        train_loss = sum(train_losses.values()) if train_losses else 0

        best_preds_dir = os.path.join(opt.checkpoints_dir, opt.name, "results")
        if not os.path.exists(best_preds_dir):
            os.makedirs(best_preds_dir)
        val_loss = AverageMeter()
        preds_all_val = []
        labels_all_val = []
        names_all_val = []
        for i, data in enumerate(val_loader):
            model.set_input(data[0], data[1], data[2], data[3], opt.gpu_ids[0])
            out_change, loss = model.get_val_pred()
            out_change = FF.interpolate(out_change, size=(512, 512), mode='bilinear', align_corners=True)
            val_loss.update(loss.cpu().detach().numpy())
            preds = torch.argmax(out_change, dim=1)
            pred_numpy = preds.cpu().numpy()
            labels_numpy = data[2].cpu().numpy()
            preds_all_val.append(pred_numpy)
            labels_all_val.append(labels_numpy)
            names_all_val.extend(data[3])
        preds_all_val = np.concatenate(preds_all_val, axis=0)
        labels_all_val = np.concatenate(labels_all_val, axis=0)
        hist = get_confuse_matrix(2, labels_all_val, preds_all_val)
        score = cm2score(hist)
        
        # è®°å½•éªŒè¯æŒ‡æ ‡åˆ°TensorBoard
        writer.add_scalar('Validation/Accuracy', score['acc'], epoch)
        writer.add_scalar('Validation/MeanIoU', score['miou'], epoch)
        writer.add_scalar('Validation/IoU_0', score['iou_0'], epoch)
        writer.add_scalar('Validation/IoU_1', score['iou_1'], epoch)
        writer.add_scalar('Validation/F1_0', score['F1_0'], epoch)
        writer.add_scalar('Validation/F1_1', score['F1_1'], epoch)
        writer.add_scalar('Validation/Loss', val_loss.average(), epoch)
        
        # è®°å½•éªŒè¯é›†ä¸Šçš„IoUï¼Œç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½
        val_iou = score['iou_1']
        
        # ä¿å­˜å½“å‰æ¨¡å‹
        model.save_networks(epoch)
        
        if val_iou > best_iou:
            with open(os.path.join(opt.checkpoints_dir, opt.name, "cd_log.txt"), 'a') as f:
                f.write(f'æ–°çºªå½•ï¼ä¿å­˜æ¨¡å‹è‡³: {os.path.join(opt.checkpoints_dir, opt.name)}\n')
            # æŸ¥æ‰¾å¹¶åˆ é™¤ä¹‹å‰çš„æœ€ä½³æ¨¡å‹æ–‡ä»¶
            for file in os.listdir(os.path.join(opt.checkpoints_dir, opt.name)):
                if file.endswith('.pth') and file.startswith('best_net'):
                    os.remove(os.path.join(opt.checkpoints_dir, opt.name, file))
            
            # ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹
            model.save_networks('best')
            best_iou = val_iou
            
            # ä¿å­˜æœ€ä½³ç»“æœé¢„æµ‹å›¾
            for i in range(len(names_all_val)):
                save_path = os.path.join(best_preds_dir, names_all_val[i])
                cv2.imwrite(save_path, preds_all_val[i] * 255)
            
            print('æ›´æ–°æœ€ä½³IoUæ¨¡å‹')
        
        # æ›´æ–°è®­ç»ƒä¿¡æ¯æ–‡ä»¶
        with open(os.path.join(opt.checkpoints_dir, opt.name, "training_info.txt"), 'w') as f:
            f.write(f"epoch:{epoch + 1}\n")  # ä¿å­˜ä¸‹ä¸€è½®æ¬¡ï¼Œä»¥ä¾¿æ–­ç‚¹ç»­è®­
            f.write(f"best_iou:{best_iou}\n")
            
        with open(os.path.join(opt.checkpoints_dir, opt.name, "cd_log.txt"), 'a') as f:
            # æ·»åŠ åˆ†éš”è¡Œ
            f.write('='*100 + '\n')
            # åˆå¹¶å±•ç¤ºè®­ç»ƒå’ŒéªŒè¯ç»“æœ
            f.write('ã€Epoch: %dã€‘è®­ç»ƒIoU: %.4f (Loss: %.4f) | éªŒè¯IoU: %.4f/%.4f (Loss: %.4f)\n' %
                   (epoch, train_iou, train_loss, score['iou_1'], best_iou, val_loss.average()))

            # # å¯¹æ¯”å±•ç¤ºå…³é”®æŒ‡æ ‡ - ä½¿ç”¨å›ºå®šå®½åº¦ç¡®ä¿å¯¹é½
            # f.write('â•”â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n')
            # f.write('â•‘ æŒ‡æ ‡å¯¹æ¯” â•‘     å‡†ç¡®ç‡     â•‘    å¹³å‡IoU     â•‘    å¹³å‡F1      â•‘\n')
            # f.write('â• â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n')
            # f.write('â•‘  è®­ç»ƒé›†  â•‘     %-7.4f   â•‘     %-7.4f   â•‘     %-7.4f   â•‘\n' %
            #        (train_score['acc'], train_score['miou'], train_score['mf1']))
            # f.write('â•‘  éªŒè¯é›†  â•‘     %-7.4f   â•‘     %-7.4f   â•‘     %-7.4f   â•‘\n' %
            #        (score['acc'], score['miou'], score['mf1']))
            # f.write('â•šâ•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n')

            # åˆ†åˆ«è®°å½•è¯¦ç»†æŒ‡æ ‡
            f.write('è®­ç»ƒè¯¦ç»†æŒ‡æ ‡: %s\n' % {k: round(v, 4) if isinstance(v, float) else v for k, v in train_score.items()})
            f.write('éªŒè¯è¯¦ç»†æŒ‡æ ‡: %s\n' % {k: round(v, 4) if isinstance(v, float) else v for k, v in score.items()})
            # f.write('='*100 + '\n\n')

        # ç¾åŒ–æ§åˆ¶å°è¾“å‡º
        print('='*100)
        # åˆå¹¶å±•ç¤ºè®­ç»ƒå’ŒéªŒè¯ç»“æœ
        print('ã€Epoch: %dã€‘è®­ç»ƒIoU: %.4f (Loss: %.4f) | éªŒè¯IoU: %.4f/%.4f (Loss: %.4f)' %
             (epoch, train_iou, train_loss, score['iou_1'], best_iou, val_loss.average()))

        # å¯¹æ¯”å±•ç¤ºå…³é”®æŒ‡æ ‡ - ä½¿ç”¨å›ºå®šå®½åº¦ç¡®ä¿å¯¹é½
        print('â•”â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—')
        print('â•‘ æŒ‡æ ‡å¯¹æ¯” â•‘     å‡†ç¡®ç‡     â•‘    å¹³å‡IoU     â•‘    å¹³å‡F1      â•‘')
        print('â• â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£')
        print('â•‘  è®­ç»ƒé›†  â•‘     %-7.4f   â•‘     %-7.4f   â•‘     %-7.4f   â•‘' %
             (train_score['acc'], train_score['miou'], train_score['mf1']))
        print('â•‘  éªŒè¯é›†  â•‘     %-7.4f   â•‘     %-7.4f   â•‘     %-7.4f   â•‘' %
             (score['acc'], score['miou'], score['mf1']))
        print('â•šâ•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•')

        # å¦‚æœéªŒè¯é›†IoUä¼˜äºä¹‹å‰æœ€ä½³å€¼ï¼Œæ˜¾ç¤ºæç¤º
        if score['iou_1'] >= best_iou - 0.0001:  # è€ƒè™‘æµ®ç‚¹ç²¾åº¦
            print('ğŸŒŸ æœ¬è½®éªŒè¯IoUåˆ›å»ºæ–°é«˜ï¼')

        print('è®­ç»ƒè½®æ¬¡ %d / %d ç»“æŸ \t è€—æ—¶: %d ç§’' % (epoch, opt.n_epochs, time.time() - epoch_start_time))

        # åœ¨æ¯ä¸ªepochç»“æŸæ—¶æ›´æ–°å­¦ä¹ ç‡
        model.update_learning_rate()
        
    # å…³é—­TensorBoardå†™å…¥å™¨
    writer.close()
